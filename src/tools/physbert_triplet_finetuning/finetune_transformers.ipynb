{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c190eb5c",
   "metadata": {},
   "source": [
    "# âš ï¸ Memory Optimization for Apple Silicon (MPS) Users\n",
    "\n",
    "If you encounter `MPS backend out of memory` errors, this notebook includes several automatic optimizations:\n",
    "\n",
    "## Automatic Optimizations Applied:\n",
    "- **Reduced Batch Size**: Automatically reduced from 8 to 4 for MPS\n",
    "- **Gradient Checkpointing**: Enabled to reduce memory usage during backpropagation\n",
    "- **Memory Cleanup**: Regular `torch.mps.empty_cache()` calls between operations\n",
    "- **Gradient Accumulation**: Maintains effective batch size through accumulation\n",
    "- **Disabled Features**: Pin memory and multiprocessing disabled for MPS stability\n",
    "\n",
    "## Manual Environment Settings (Optional):\n",
    "If you still encounter memory issues, run these commands in terminal before training:\n",
    "\n",
    "```bash\n",
    "# Disable MPS memory limit (may cause system instability)\n",
    "export PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0\n",
    "\n",
    "# Further reduce batch size\n",
    "export PHYSBERT_BATCH_SIZE=2\n",
    "\n",
    "# Reduce training examples for testing\n",
    "export PHYSBERT_TEST_SIZE=5\n",
    "\n",
    "# Skip data augmentation to reduce memory usage\n",
    "export PHYSBERT_SKIP_AUGMENTATION=true\n",
    "```\n",
    "\n",
    "## Memory-Friendly Training Settings:\n",
    "- **Small Dataset**: Use `PHYSBERT_TEST_SIZE=10` for initial testing\n",
    "- **Shorter Sequences**: Model automatically uses 512 max length\n",
    "- **Single Process**: No multiprocessing to avoid memory conflicts\n",
    "- **Evaluation Batching**: Reduced batch sizes during model comparison\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4c1641",
   "metadata": {},
   "source": [
    "# PhysBERT Transformers-Based Fine-tuning Pipeline for Physics Domain Specialization\n",
    "\n",
    "## Overview\n",
    "This notebook fine-tunes the **PhysBERT** model (`thellert/physbert_cased`) on physics-specific triplet data using the **transformers** library directly (NOT sentence-transformers). The goal is to create specialized embeddings that better understand physics concepts and relationships through contrastive learning.\n",
    "\n",
    "## Architecture Strategy\n",
    "- **Base Model**: PhysBERT (`thellert/physbert_cased`) - BERT model pre-trained on physics literature\n",
    "- **Fine-tuning Method**: Native transformers with custom triplet loss and training loop\n",
    "- **Training Data**: Query-Positive-Negative triplets from physics textbook content\n",
    "- **Loss Function**: Triplet loss with margin-based contrastive learning\n",
    "- **Output**: Physics-specialized embedding model for domain-specific tasks\n",
    "\n",
    "## Key Differences from Sentence-Transformers Approach\n",
    "\n",
    "### Transformers-Based Approach (This Notebook):\n",
    "- **Direct Model Control**: Full access to model architecture and training loop\n",
    "- **Custom Loss Implementation**: Hand-crafted triplet loss with gradient computation\n",
    "- **Flexible Pooling**: Custom mean pooling implementation for embeddings\n",
    "- **Training Loop**: Manual forward/backward pass with optimizer control\n",
    "- **Memory Efficiency**: Better control over batch processing and gradient accumulation\n",
    "\n",
    "### Sentence-Transformers Approach (Original):\n",
    "- **High-level API**: Built-in triplet training with automatic handling\n",
    "- **Less Control**: Limited customization of training process\n",
    "- **Simplified Setup**: Easy configuration but less flexibility\n",
    "\n",
    "## Files and Dependencies\n",
    "\n",
    "### Input Files:\n",
    "- `data/physics_triplets_st.json` - Triplet training data in format: `{\"texts\": [query, positive, negative]}`\n",
    "- `config/physbert_config.yaml` - Training configuration and hyperparameters\n",
    "- `main.tex` - Original LaTeX file with physics content (source for triplets)\n",
    "\n",
    "### Key Dependencies:\n",
    "```python\n",
    "transformers>=4.21.0  # For PhysBERT model and tokenizer\n",
    "torch>=1.12.0         # PyTorch for model training\n",
    "datasets>=2.0.0       # Data handling and processing\n",
    "accelerate>=0.20.0    # Training acceleration\n",
    "scikit-learn>=1.1.0   # For evaluation metrics\n",
    "pyyaml>=6.0          # Configuration management\n",
    "```\n",
    "\n",
    "### Output Files:\n",
    "- `models/physbert-transformers-finetuned/` - Fine-tuned model directory\n",
    "- `logs/physbert_transformers_training.log` - Training logs and metrics\n",
    "- `models/training_metadata_transformers.yaml` - Training configuration and results\n",
    "\n",
    "## Training Process Flow\n",
    "\n",
    "1. **Data Loading**: Load physics triplets from JSON with proper validation\n",
    "2. **Model Setup**: Load PhysBERT using transformers AutoModel/AutoTokenizer\n",
    "3. **Custom Embedding Layer**: Implement mean pooling for generating embeddings\n",
    "4. **Triplet Loss Implementation**: Custom triplet loss function with margin\n",
    "5. **Training Loop**: Manual forward/backward pass with optimizer control\n",
    "6. **Data Augmentation**: Generate additional triplets via negative shuffling\n",
    "7. **Evaluation**: Compare base vs fine-tuned model on physics queries\n",
    "8. **Model Saving**: Export fine-tuned model for downstream physics tasks\n",
    "\n",
    "## Expected Improvements\n",
    "\n",
    "### Base PhysBERT Performance:\n",
    "- General physics understanding from pre-training\n",
    "- Good but not optimized for specific textbook concepts\n",
    "\n",
    "### Fine-tuned Model Performance:\n",
    "- **Higher query-positive similarity** for physics concepts\n",
    "- **Better separation** between relevant and irrelevant physics content  \n",
    "- **Domain-specific optimization** for textbook chapter material\n",
    "- **Improved retrieval** for physics Q&A and concept matching\n",
    "- **Lower-level control** over embedding generation process\n",
    "\n",
    "## Usage Example\n",
    "\n",
    "```python\n",
    "# Load fine-tuned model\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"models/physbert-transformers-finetuned\")\n",
    "model = AutoModel.from_pretrained(\"models/physbert-transformers-finetuned\")\n",
    "\n",
    "# Generate embeddings for physics queries\n",
    "def get_embeddings(texts):\n",
    "    inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        embeddings = outputs.last_hidden_state.mean(dim=1)  # Mean pooling\n",
    "    return embeddings\n",
    "\n",
    "query = \"What describes energy loss of charged particles in matter?\"\n",
    "embeddings = get_embeddings([query])\n",
    "```\n",
    "\n",
    "## Configuration\n",
    "\n",
    "Training parameters are managed through `config/physbert_config.yaml` following RAGAS framework patterns with environment variable overrides:\n",
    "\n",
    "- `PHYSBERT_BATCH_SIZE`: Override training batch size\n",
    "- `PHYSBERT_EPOCHS`: Override number of training epochs  \n",
    "- `PHYSBERT_OUTPUT_PATH`: Override model save directory\n",
    "- `PHYSBERT_TEST_SIZE`: Limit training data for testing\n",
    "- `PHYSBERT_LEARNING_RATE`: Override learning rate\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f22e7705",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sandeshs/Documents/Projects/LLMTest/venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW  # Fixed: Use torch.optim.AdamW instead of transformers.AdamW\n",
    "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import gc  # For garbage collection\n",
    "\n",
    "# Setup logging with handler management to prevent duplicates\n",
    "def setup_logging():\n",
    "    logger = logging.getLogger(__name__)\n",
    "    \n",
    "    # Clear existing handlers to prevent duplicates\n",
    "    if logger.handlers:\n",
    "        logger.handlers.clear()\n",
    "    \n",
    "    # Only add handler if none exists\n",
    "    if not logger.handlers:\n",
    "        handler = logging.StreamHandler()\n",
    "        formatter = logging.Formatter('%(levelname)s: %(message)s')\n",
    "        handler.setFormatter(formatter)\n",
    "        logger.addHandler(handler)\n",
    "        logger.setLevel(logging.INFO)\n",
    "    \n",
    "    return logger\n",
    "\n",
    "logger = setup_logging()\n",
    "\n",
    "def check_mps_memory():\n",
    "    \"\"\"\n",
    "    Check MPS memory usage and provide recommendations\n",
    "    \"\"\"\n",
    "    if torch.backends.mps.is_available():\n",
    "        try:\n",
    "            # Get memory info (this may not work on all macOS versions)\n",
    "            print(\"ðŸŽ MPS Memory Status:\")\n",
    "            print(f\"   - MPS Available: {torch.backends.mps.is_available()}\")\n",
    "            print(f\"   - MPS Built: {torch.backends.mps.is_built()}\")\n",
    "            \n",
    "            # Force garbage collection\n",
    "            gc.collect()\n",
    "            torch.mps.empty_cache()\n",
    "            print(\"Memory cache cleared\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not get MPS memory info: {e}\")\n",
    "    else:\n",
    "        print(\"MPS not available on this system\")\n",
    "\n",
    "def optimize_for_memory():\n",
    "    \"\"\"\n",
    "    Apply memory optimization settings for training (removed watermark ratio)\n",
    "    \"\"\"\n",
    "    print(\"ðŸ”§ Applied memory optimizations:\")\n",
    "    \n",
    "    if torch.backends.mps.is_available():\n",
    "        torch.mps.empty_cache()\n",
    "        print(\"- MPS cache cleared\")\n",
    "    \n",
    "    gc.collect()\n",
    "    print(\"- Garbage collection completed\")\n",
    "\n",
    "def load_triplets_from_json(json_path: str = \"data/physics_triplets_st.json\") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Load physics triplets from JSON for transformers-based training\n",
    "    \n",
    "    Args:\n",
    "        json_path: Path to the physics triplets JSON file\n",
    "        \n",
    "    Returns:\n",
    "        List of dictionaries with triplet data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        config_path = os.getenv('TRIPLET_DATA_PATH', json_path)\n",
    "        \n",
    "        with open(config_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        triplets = []\n",
    "        for item in data:\n",
    "            if 'texts' in item and len(item['texts']) == 3:\n",
    "                triplet = {\n",
    "                    'anchor': item['texts'][0],    # query\n",
    "                    'positive': item['texts'][1],  # relevant text\n",
    "                    'negative': item['texts'][2],  # irrelevant text\n",
    "                    'id': item.get('id', f'triplet_{len(triplets)}')\n",
    "                }\n",
    "                triplets.append(triplet)\n",
    "        \n",
    "        print(f\"Loaded {len(triplets)} triplet examples for training\")\n",
    "        return triplets\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Physics triplet file not found: {json_path}\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading triplets: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "907dfe41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_physics_triplets(triplets: List[Dict], \n",
    "                           negative_shuffle_factor: int = 2,\n",
    "                           hard_negative_factor: int = 1) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Augment physics triplets using negative shuffling and hard negatives\n",
    "    \n",
    "    Args:\n",
    "        triplets: Original triplet dictionaries\n",
    "        negative_shuffle_factor: Number of shuffled negatives per original\n",
    "        hard_negative_factor: Number of hard negatives per original\n",
    "        \n",
    "    Returns:\n",
    "        Augmented list of triplet dictionaries\n",
    "    \"\"\"\n",
    "    augmented = triplets.copy()  # Keep originals\n",
    "    \n",
    "    # Extract all negatives and positives\n",
    "    all_negatives = [t['negative'] for t in triplets]\n",
    "    all_positives = [t['positive'] for t in triplets]\n",
    "    \n",
    "    for i, triplet in enumerate(triplets):\n",
    "        anchor = triplet['anchor']\n",
    "        positive = triplet['positive']\n",
    "        \n",
    "        # Negative shuffling: use other negatives\n",
    "        other_negatives = [neg for j, neg in enumerate(all_negatives) if j != i]\n",
    "        for shuffle_idx in range(min(negative_shuffle_factor, len(other_negatives))):\n",
    "            shuffled_neg = random.choice(other_negatives)\n",
    "            other_negatives.remove(shuffled_neg)\n",
    "            \n",
    "            augmented.append({\n",
    "                'anchor': anchor,\n",
    "                'positive': positive,\n",
    "                'negative': shuffled_neg,\n",
    "                'id': f\"{triplet['id']}_shuffle_{shuffle_idx}\"\n",
    "            })\n",
    "        \n",
    "        # Hard negatives: use other positives as challenging negatives\n",
    "        other_positives = [pos for j, pos in enumerate(all_positives) if j != i]\n",
    "        for hard_idx in range(min(hard_negative_factor, len(other_positives))):\n",
    "            hard_neg = random.choice(other_positives)\n",
    "            other_positives.remove(hard_neg)\n",
    "            \n",
    "            augmented.append({\n",
    "                'anchor': anchor,\n",
    "                'positive': positive, \n",
    "                'negative': hard_neg,\n",
    "                'id': f\"{triplet['id']}_hard_{hard_idx}\"\n",
    "            })\n",
    "    \n",
    "    print(f\"Data augmentation: {len(triplets)} -> {len(augmented)} examples\")\n",
    "    return augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65e7ec6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for triplet training with transformers\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, triplets: List[Dict], tokenizer, max_length: int = 512):\n",
    "        self.triplets = triplets\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.triplets)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        triplet = self.triplets[idx]\n",
    "        \n",
    "        # Tokenize all three texts\n",
    "        anchor_inputs = self.tokenizer(\n",
    "            triplet['anchor'],\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        positive_inputs = self.tokenizer(\n",
    "            triplet['positive'],\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        negative_inputs = self.tokenizer(\n",
    "            triplet['negative'],\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'anchor_input_ids': anchor_inputs['input_ids'].squeeze(),\n",
    "            'anchor_attention_mask': anchor_inputs['attention_mask'].squeeze(),\n",
    "            'positive_input_ids': positive_inputs['input_ids'].squeeze(),\n",
    "            'positive_attention_mask': positive_inputs['attention_mask'].squeeze(),\n",
    "            'negative_input_ids': negative_inputs['input_ids'].squeeze(),\n",
    "            'negative_attention_mask': negative_inputs['attention_mask'].squeeze(),\n",
    "            'triplet_id': triplet['id']\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "548a6e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhysBERTEmbeddingModel(nn.Module):\n",
    "    \"\"\"\n",
    "    PhysBERT model wrapper for generating embeddings with custom pooling\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"thellert/physbert_cased\"):\n",
    "        super(PhysBERTEmbeddingModel, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        self.pooling_strategy = 'mean'  # Can be 'mean', 'cls', 'max'\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        \"\"\"\n",
    "        Forward pass to generate embeddings\n",
    "        \n",
    "        Args:\n",
    "            input_ids: Token IDs\n",
    "            attention_mask: Attention mask\n",
    "            \n",
    "        Returns:\n",
    "            Pooled embeddings\n",
    "        \"\"\"\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        if self.pooling_strategy == 'mean':\n",
    "            # Mean pooling over sequence length\n",
    "            token_embeddings = outputs.last_hidden_state\n",
    "            attention_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "            sum_embeddings = torch.sum(token_embeddings * attention_mask_expanded, 1)\n",
    "            sum_mask = torch.clamp(attention_mask_expanded.sum(1), min=1e-9)\n",
    "            embeddings = sum_embeddings / sum_mask\n",
    "            \n",
    "        elif self.pooling_strategy == 'cls':\n",
    "            # Use [CLS] token embedding\n",
    "            embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "            \n",
    "        elif self.pooling_strategy == 'max':\n",
    "            # Max pooling over sequence length\n",
    "            token_embeddings = outputs.last_hidden_state\n",
    "            attention_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "            token_embeddings[attention_mask_expanded == 0] = -1e9  # Set padding tokens to very negative values\n",
    "            embeddings = torch.max(token_embeddings, 1)[0]\n",
    "            \n",
    "        return embeddings\n",
    "    \n",
    "    def encode(self, texts: List[str], tokenizer, device, batch_size: int = 32):\n",
    "        \"\"\"\n",
    "        Encode texts to embeddings (for evaluation) with memory optimization\n",
    "        \n",
    "        Args:\n",
    "            texts: List of texts to encode\n",
    "            tokenizer: Tokenizer instance\n",
    "            device: Device to run on\n",
    "            batch_size: Batch size for encoding\n",
    "            \n",
    "        Returns:\n",
    "            Numpy array of embeddings\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        embeddings = []\n",
    "        \n",
    "        # Memory optimization: Reduce batch size for MPS\n",
    "        if device.type == 'mps':\n",
    "            batch_size = min(batch_size, 16)  # Smaller batches for MPS\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i in range(0, len(texts), batch_size):\n",
    "                batch_texts = texts[i:i+batch_size]\n",
    "                \n",
    "                inputs = tokenizer(\n",
    "                    batch_texts,\n",
    "                    max_length=512,\n",
    "                    padding=True,\n",
    "                    truncation=True,\n",
    "                    return_tensors='pt'\n",
    "                ).to(device, non_blocking=False)  # Disable non_blocking for MPS\n",
    "                \n",
    "                batch_embeddings = self.forward(\n",
    "                    inputs['input_ids'],\n",
    "                    inputs['attention_mask']\n",
    "                )\n",
    "                \n",
    "                embeddings.append(batch_embeddings.cpu().numpy())\n",
    "                \n",
    "                # Memory cleanup for MPS after each batch\n",
    "                if device.type == 'mps':\n",
    "                    torch.mps.empty_cache()\n",
    "                elif device.type == 'cuda':\n",
    "                    torch.cuda.empty_cache()\n",
    "                \n",
    "                # Clear input tensors\n",
    "                del inputs, batch_embeddings\n",
    "        \n",
    "        return np.vstack(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "181a02f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Custom triplet loss implementation for contrastive learning\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, margin: float = 0.5, distance_metric: str = 'cosine'):\n",
    "        super(TripletLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "        self.distance_metric = distance_metric.lower()\n",
    "        \n",
    "    def forward(self, anchor_embeddings, positive_embeddings, negative_embeddings):\n",
    "        \"\"\"\n",
    "        Compute triplet loss\n",
    "        \n",
    "        Args:\n",
    "            anchor_embeddings: Anchor embeddings\n",
    "            positive_embeddings: Positive embeddings\n",
    "            negative_embeddings: Negative embeddings\n",
    "            \n",
    "        Returns:\n",
    "            Triplet loss value\n",
    "        \"\"\"\n",
    "        if self.distance_metric == 'cosine':\n",
    "            # Use cosine similarity (higher is better)\n",
    "            pos_similarity = F.cosine_similarity(anchor_embeddings, positive_embeddings, dim=1)\n",
    "            neg_similarity = F.cosine_similarity(anchor_embeddings, negative_embeddings, dim=1)\n",
    "            \n",
    "            # For cosine similarity, we want: pos_sim > neg_sim + margin\n",
    "            # So loss = max(0, margin - (pos_sim - neg_sim))\n",
    "            loss = F.relu(self.margin - (pos_similarity - neg_similarity))\n",
    "            \n",
    "        elif self.distance_metric == 'euclidean':\n",
    "            # Use Euclidean distance (lower is better)\n",
    "            pos_distance = F.pairwise_distance(anchor_embeddings, positive_embeddings, p=2)\n",
    "            neg_distance = F.pairwise_distance(anchor_embeddings, negative_embeddings, p=2)\n",
    "            \n",
    "            # For euclidean distance, we want: pos_dist + margin < neg_dist\n",
    "            # So loss = max(0, pos_dist - neg_dist + margin)\n",
    "            loss = F.relu(pos_distance - neg_distance + self.margin)\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported distance metric: {self.distance_metric}\")\n",
    "        \n",
    "        return loss.mean()\n",
    "    \n",
    "    def get_similarities(self, anchor_embeddings, positive_embeddings, negative_embeddings):\n",
    "        \"\"\"\n",
    "        Get similarity scores for evaluation\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with positive and negative similarities\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            if self.distance_metric == 'cosine':\n",
    "                pos_sim = F.cosine_similarity(anchor_embeddings, positive_embeddings, dim=1)\n",
    "                neg_sim = F.cosine_similarity(anchor_embeddings, negative_embeddings, dim=1)\n",
    "                return {\n",
    "                    'positive_similarity': pos_sim.mean().item(),\n",
    "                    'negative_similarity': neg_sim.mean().item(),\n",
    "                    'margin': (pos_sim - neg_sim).mean().item()\n",
    "                }\n",
    "            else:\n",
    "                pos_dist = F.pairwise_distance(anchor_embeddings, positive_embeddings, p=2)\n",
    "                neg_dist = F.pairwise_distance(anchor_embeddings, negative_embeddings, p=2)\n",
    "                return {\n",
    "                    'positive_distance': pos_dist.mean().item(),\n",
    "                    'negative_distance': neg_dist.mean().item(),\n",
    "                    'margin': (neg_dist - pos_dist).mean().item()\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bdf014cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_training_config(config_path: str) -> dict:\n",
    "    \"\"\"\n",
    "    Load training configuration with fallback defaults for transformers training\n",
    "    \"\"\"\n",
    "    default_config = {\n",
    "        'model': {'base_model': 'thellert/physbert_cased'},\n",
    "        'data': {'path': 'data/physics_triplets_st.json'},\n",
    "        'augmentation': {\n",
    "            'enabled': True,\n",
    "            'negative_shuffle_factor': 2, \n",
    "            'hard_negative_factor': 1\n",
    "        },\n",
    "        'training': {\n",
    "            'batch_size': 8,\n",
    "            'epochs': 3,\n",
    "            'learning_rate': 2e-5,\n",
    "            'warmup_ratio': 0.1,\n",
    "            'weight_decay': 0.01,\n",
    "            'max_length': 512,\n",
    "            'gradient_accumulation_steps': 1,\n",
    "            'output_path': 'models/physbert-transformers-finetuned',\n",
    "            'save_steps': 100,\n",
    "            'eval_steps': 50,\n",
    "            'triplet_loss': {\n",
    "                'distance_metric': 'cosine', \n",
    "                'margin': 0.5\n",
    "            }\n",
    "        },\n",
    "        'evaluation': {\n",
    "            'enabled': True,\n",
    "            'test_ratio': 0.2,\n",
    "            'comparison': {\n",
    "                'enabled': True,\n",
    "                'sample_queries': 5\n",
    "            }\n",
    "        },\n",
    "        'logging': {\n",
    "            'level': 'INFO',\n",
    "            'save_logs': True,\n",
    "            'log_file': 'logs/physbert_transformers_training.log'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        if os.path.exists(config_path):\n",
    "            with open(config_path, 'r') as f:\n",
    "                user_config = yaml.safe_load(f)\n",
    "            # Deep merge configs with user overrides\n",
    "            def deep_merge(default, user):\n",
    "                for key, value in user.items():\n",
    "                    if key in default and isinstance(default[key], dict) and isinstance(value, dict):\n",
    "                        deep_merge(default[key], value)\n",
    "                    else:\n",
    "                        default[key] = value\n",
    "                return default\n",
    "            \n",
    "            return deep_merge(default_config, user_config)\n",
    "        else:\n",
    "            print(f\"âš ï¸  Config file not found: {config_path}, using defaults\")\n",
    "            return default_config\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸  Error loading config: {e}, using defaults\")\n",
    "        return default_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4382688f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_physbert_transformers(config_path: str = \"config/physbert_config.yaml\"):\n",
    "    \"\"\"\n",
    "    Main training function for PhysBERT fine-tuning using transformers\n",
    "    \n",
    "    Args:\n",
    "        config_path: Path to training configuration file\n",
    "        \n",
    "    Returns:\n",
    "        Trained model and tokenizer\n",
    "    \"\"\"\n",
    "    # Configuration-driven setup following RAGAS patterns\n",
    "    config = _load_training_config(config_path)\n",
    "    \n",
    "    print(\"PhysBERT Transformers Fine-tuning Pipeline\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Set device with Apple Silicon MPS support and memory optimization\n",
    "    if torch.backends.mps.is_available():\n",
    "        device = torch.device('mps')\n",
    "        print(f\"Using Apple Silicon MPS: {device}\")\n",
    "        torch.mps.empty_cache()\n",
    "    elif torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "        print(f\"Using CUDA GPU: {device}\")\n",
    "        torch.cuda.empty_cache()\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "        print(f\"Using CPU: {device}\")\n",
    "    \n",
    "    try:\n",
    "        # Load base model and tokenizer\n",
    "        base_model_name = config.get('model', {}).get('base_model', 'thellert/physbert_cased')\n",
    "        print(f\"Loading base model: {base_model_name}\")\n",
    "        \n",
    "        tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "        model = PhysBERTEmbeddingModel(base_model_name).to(device)\n",
    "        \n",
    "        # Enable gradient checkpointing for memory efficiency\n",
    "        if hasattr(model.bert, 'gradient_checkpointing_enable'):\n",
    "            model.bert.gradient_checkpointing_enable()\n",
    "            print(\"Enabled gradient checkpointing\")\n",
    "        \n",
    "        print(\"Model and tokenizer loaded successfully\")\n",
    "        \n",
    "        # Load training data\n",
    "        data_path = os.getenv('TRIPLET_DATA_PATH', config.get('data', {}).get('path', 'data/physics_triplets_st.json'))\n",
    "        triplets = load_triplets_from_json(data_path)\n",
    "        \n",
    "        if not triplets:\n",
    "            raise ValueError(f\"No training examples loaded from {data_path}\")\n",
    "        \n",
    "        # Limit examples for demo/test runs\n",
    "        test_limit = int(os.getenv('PHYSBERT_TEST_SIZE', len(triplets)))\n",
    "        triplets = triplets[:test_limit]\n",
    "        print(f\"Using {len(triplets)} examples for training\")\n",
    "        \n",
    "        # Data augmentation with config-driven parameters\n",
    "        aug_config = config.get('augmentation', {})\n",
    "        if aug_config.get('enabled', True) and os.getenv('PHYSBERT_SKIP_AUGMENTATION') != 'true':\n",
    "            augmented_triplets = augment_physics_triplets(\n",
    "                triplets,\n",
    "                negative_shuffle_factor=aug_config.get('negative_shuffle_factor', 2),\n",
    "                hard_negative_factor=aug_config.get('hard_negative_factor', 1)\n",
    "            )\n",
    "        else:\n",
    "            augmented_triplets = triplets\n",
    "            print(\"Data augmentation disabled\")\n",
    "        \n",
    "        # Create dataset and dataloader with memory-optimized settings\n",
    "        train_config = config.get('training', {})\n",
    "        max_length = train_config.get('max_length', 512)\n",
    "        \n",
    "        # Memory optimization: Reduce batch size for MPS\n",
    "        original_batch_size = int(os.getenv('PHYSBERT_BATCH_SIZE', train_config.get('batch_size', 8)))\n",
    "        if device.type == 'mps':\n",
    "            batch_size = min(original_batch_size, 4)\n",
    "            print(f\"MPS optimization: batch size reduced from {original_batch_size} to {batch_size}\")\n",
    "        else:\n",
    "            batch_size = original_batch_size\n",
    "        \n",
    "        dataset = TripletDataset(augmented_triplets, tokenizer, max_length=max_length)\n",
    "        dataloader = DataLoader(\n",
    "            dataset, \n",
    "            batch_size=batch_size, \n",
    "            shuffle=True,\n",
    "            pin_memory=False if device.type == 'mps' else True,  # Disable pin_memory for MPS\n",
    "            num_workers=0  # Disable multiprocessing for MPS stability\n",
    "        )\n",
    "        \n",
    "        # Setup loss function\n",
    "        triplet_config = train_config.get('triplet_loss', {})\n",
    "        criterion = TripletLoss(\n",
    "            margin=triplet_config.get('margin', 0.5),\n",
    "            distance_metric=triplet_config.get('distance_metric', 'cosine')\n",
    "        )\n",
    "        \n",
    "        # Setup optimizer and scheduler\n",
    "        learning_rate = float(os.getenv('PHYSBERT_LEARNING_RATE', train_config.get('learning_rate', 2e-5)))\n",
    "        weight_decay = train_config.get('weight_decay', 0.01)\n",
    "        epochs = int(os.getenv('PHYSBERT_EPOCHS', train_config.get('epochs', 3)))\n",
    "        \n",
    "        optimizer = AdamW(\n",
    "            model.parameters(),\n",
    "            lr=learning_rate,\n",
    "            weight_decay=weight_decay\n",
    "        )\n",
    "        \n",
    "        total_steps = len(dataloader) * epochs\n",
    "        warmup_steps = int(total_steps * train_config.get('warmup_ratio', 0.1))\n",
    "        \n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=warmup_steps,\n",
    "            num_training_steps=total_steps\n",
    "        )\n",
    "        \n",
    "        # Memory optimization: Enable gradient accumulation if batch size was reduced\n",
    "        gradient_accumulation_steps = max(1, original_batch_size // batch_size)\n",
    "        \n",
    "        print(f\"ðŸš€ Training Configuration:\")\n",
    "        print(f\"   - Examples: {len(augmented_triplets)}\")\n",
    "        print(f\"   - Batch size: {batch_size} (gradient accumulation: {gradient_accumulation_steps})\")\n",
    "        print(f\"   - Epochs: {epochs}\")\n",
    "        print(f\"   - Learning rate: {learning_rate}\")\n",
    "        print(f\"   - Total steps: {total_steps}\")\n",
    "        print(f\"   - Warmup steps: {warmup_steps}\")\n",
    "        print(f\"   - Max length: {max_length}\")\n",
    "        print(f\"   - Device: {device}\")\n",
    "        \n",
    "        # Training loop with memory optimization\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        step_count = 0\n",
    "        accumulated_loss = 0\n",
    "        \n",
    "        print(\"\\nStarting training...\")\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
    "            epoch_loss = 0\n",
    "            \n",
    "            progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch + 1}\")\n",
    "            \n",
    "            for batch_idx, batch in enumerate(progress_bar):\n",
    "                # Move batch to device\n",
    "                anchor_input_ids = batch['anchor_input_ids'].to(device, non_blocking=False)\n",
    "                anchor_attention_mask = batch['anchor_attention_mask'].to(device, non_blocking=False)\n",
    "                positive_input_ids = batch['positive_input_ids'].to(device, non_blocking=False)\n",
    "                positive_attention_mask = batch['positive_attention_mask'].to(device, non_blocking=False)\n",
    "                negative_input_ids = batch['negative_input_ids'].to(device, non_blocking=False)\n",
    "                negative_attention_mask = batch['negative_attention_mask'].to(device, non_blocking=False)\n",
    "                \n",
    "                # Forward pass\n",
    "                anchor_embeddings = model(anchor_input_ids, anchor_attention_mask)\n",
    "                positive_embeddings = model(positive_input_ids, positive_attention_mask)\n",
    "                negative_embeddings = model(negative_input_ids, negative_attention_mask)\n",
    "                \n",
    "                # Compute loss\n",
    "                loss = criterion(anchor_embeddings, positive_embeddings, negative_embeddings)\n",
    "                \n",
    "                # Scale loss for gradient accumulation\n",
    "                loss = loss / gradient_accumulation_steps\n",
    "                accumulated_loss += loss.item()\n",
    "                \n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "                \n",
    "                # Only step optimizer after accumulating gradients\n",
    "                if (batch_idx + 1) % gradient_accumulation_steps == 0 or (batch_idx + 1) == len(dataloader):\n",
    "                    optimizer.step()\n",
    "                    scheduler.step()\n",
    "                    optimizer.zero_grad()\n",
    "                    \n",
    "                    # Update metrics\n",
    "                    total_loss += accumulated_loss\n",
    "                    epoch_loss += accumulated_loss\n",
    "                    step_count += 1\n",
    "                    accumulated_loss = 0\n",
    "                    \n",
    "                    # Memory cleanup for MPS\n",
    "                    if device.type == 'mps' and step_count % 10 == 0:\n",
    "                        torch.mps.empty_cache()\n",
    "                \n",
    "                # Update progress bar\n",
    "                progress_bar.set_postfix({\n",
    "                    'loss': f'{loss.item() * gradient_accumulation_steps:.4f}',\n",
    "                    'avg_loss': f'{epoch_loss / max(1, (batch_idx + 1) // gradient_accumulation_steps):.4f}',\n",
    "                    'lr': f'{scheduler.get_last_lr()[0]:.2e}',\n",
    "                    'device': str(device),\n",
    "                    'mem_opt': 'ON' if device.type == 'mps' else 'OFF'\n",
    "                })\n",
    "                \n",
    "                # Evaluation step\n",
    "                if step_count % train_config.get('eval_steps', 50) == 0:\n",
    "                    with torch.no_grad():  # Reduce memory usage during evaluation\n",
    "                        similarities = criterion.get_similarities(\n",
    "                            anchor_embeddings, positive_embeddings, negative_embeddings\n",
    "                        )\n",
    "                        print(f\"\\nStep {step_count} similarities: {similarities}\")\n",
    "            \n",
    "            # Memory cleanup after epoch\n",
    "            if device.type == 'mps':\n",
    "                torch.mps.empty_cache()\n",
    "            elif device.type == 'cuda':\n",
    "                torch.cuda.empty_cache()\n",
    "            \n",
    "            avg_epoch_loss = epoch_loss / max(1, len(dataloader) // gradient_accumulation_steps)\n",
    "            print(f\"Epoch {epoch + 1} average loss: {avg_epoch_loss:.4f}\")\n",
    "        \n",
    "        avg_total_loss = total_loss / step_count\n",
    "        print(f\"\\nTraining completed. Average loss: {avg_total_loss:.4f}\")\n",
    "        \n",
    "        # Save the fine-tuned model\n",
    "        output_path = os.getenv('PHYSBERT_OUTPUT_PATH', train_config.get('output_path', 'models/physbert-transformers-finetuned'))\n",
    "        os.makedirs(output_path, exist_ok=True)\n",
    "        \n",
    "        # Save model and tokenizer\n",
    "        model.bert.save_pretrained(output_path)\n",
    "        tokenizer.save_pretrained(output_path)\n",
    "        \n",
    "        # Save model wrapper state\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'pooling_strategy': model.pooling_strategy,\n",
    "            'training_config': config,\n",
    "            'final_loss': avg_total_loss\n",
    "        }, os.path.join(output_path, 'model_wrapper.pt'))\n",
    "        \n",
    "        print(f\"Model saved to: {output_path}\")\n",
    "        \n",
    "        # Save training metadata\n",
    "        _save_training_metadata_transformers(config, len(augmented_triplets), output_path, avg_total_loss)\n",
    "        \n",
    "        # Final memory cleanup\n",
    "        if device.type == 'mps':\n",
    "            torch.mps.empty_cache()\n",
    "        elif device.type == 'cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        return model, tokenizer, config\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Training failed: {str(e)}\")\n",
    "        # Emergency memory cleanup\n",
    "        if device.type == 'mps':\n",
    "            torch.mps.empty_cache()\n",
    "        elif device.type == 'cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "        raise\n",
    "\n",
    "def _save_training_metadata_transformers(config: dict, num_examples: int, output_path: str, final_loss: float):\n",
    "    \"\"\"\n",
    "    Save training metadata following RAGAS results management patterns\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get device info with MPS support\n",
    "        if torch.backends.mps.is_available():\n",
    "            device_info = f\"mps ({torch.backends.mps.is_built()})\"\n",
    "        elif torch.cuda.is_available():\n",
    "            device_info = f\"cuda ({torch.cuda.get_device_name()})\"\n",
    "        else:\n",
    "            device_info = \"cpu\"\n",
    "            \n",
    "        metadata = {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'config': config,\n",
    "            'training_examples': num_examples,\n",
    "            'model_path': output_path,\n",
    "            'framework': 'physbert_transformers_finetuning',\n",
    "            'version': '1.0',\n",
    "            'final_loss': final_loss,\n",
    "            'device': device_info,\n",
    "            'device_type': 'apple_silicon_mps' if torch.backends.mps.is_available() else 'other'\n",
    "        }\n",
    "        \n",
    "        metadata_path = os.path.join(output_path, 'training_metadata_transformers.yaml')\n",
    "        \n",
    "        with open(metadata_path, 'w') as f:\n",
    "            yaml.dump(metadata, f, default_flow_style=False)\n",
    "        \n",
    "        print(f\"Training metadata saved to {metadata_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Failed to save metadata: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "610f95bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_finetuned_model(model_path: str, device=None):\n",
    "    \"\"\"\n",
    "    Load a fine-tuned PhysBERT model\n",
    "    \n",
    "    Args:\n",
    "        model_path: Path to the fine-tuned model directory\n",
    "        device: Device to load model on\n",
    "        \n",
    "    Returns:\n",
    "        Loaded model, tokenizer, and config\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        # Auto-select device with Apple Silicon MPS support\n",
    "        if torch.backends.mps.is_available():\n",
    "            device = torch.device('mps')\n",
    "        elif torch.cuda.is_available():\n",
    "            device = torch.device('cuda')\n",
    "        else:\n",
    "            device = torch.device('cpu')\n",
    "    \n",
    "    try:\n",
    "        # Load tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        \n",
    "        # Load model wrapper state\n",
    "        wrapper_path = os.path.join(model_path, 'model_wrapper.pt')\n",
    "        if os.path.exists(wrapper_path):\n",
    "            checkpoint = torch.load(wrapper_path, map_location=device)\n",
    "            \n",
    "            # Create model instance\n",
    "            model = PhysBERTEmbeddingModel()\n",
    "            model.bert = AutoModel.from_pretrained(model_path)\n",
    "            model.pooling_strategy = checkpoint.get('pooling_strategy', 'mean')\n",
    "            \n",
    "            # Load state dict\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            model.to(device)\n",
    "            \n",
    "            config = checkpoint.get('training_config', {})\n",
    "            \n",
    "            print(f\"Loaded fine-tuned model from {model_path} on {device}\")\n",
    "            return model, tokenizer, config\n",
    "        else:\n",
    "            print(f\"Model wrapper not found, loading as base model\")\n",
    "            model = PhysBERTEmbeddingModel()\n",
    "            model.bert = AutoModel.from_pretrained(model_path)\n",
    "            model.to(device)\n",
    "            print(f\"Loaded base model on {device}\")\n",
    "            return model, tokenizer, {}\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd4ef521",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models_performance_transformers(base_model_name: str, \n",
    "                                          finetuned_model_path: str, \n",
    "                                          test_triplets: List[Dict], \n",
    "                                          num_samples: int = 3,\n",
    "                                          save_results: bool = True,\n",
    "                                          results_file: str = \"model_comparison_results.json\"):\n",
    "    \"\"\"\n",
    "    Compare base model vs fine-tuned model performance using transformers with memory optimization\n",
    "    \n",
    "    Args:\n",
    "        base_model_name: Name/path of base model\n",
    "        finetuned_model_path: Path to fine-tuned model\n",
    "        test_triplets: List of triplet dictionaries for testing\n",
    "        num_samples: Number of examples to test\n",
    "        save_results: Whether to save detailed results to JSON\n",
    "        results_file: Filename for saving JSON results\n",
    "    \"\"\"\n",
    "    # Set device with Apple Silicon MPS support\n",
    "    if torch.backends.mps.is_available():\n",
    "        device = torch.device('mps')\n",
    "        print(f\"Using Apple Silicon MPS for comparison\")\n",
    "        torch.mps.empty_cache()\n",
    "    elif torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "        print(f\"Using CUDA GPU for comparison\")\n",
    "        torch.cuda.empty_cache()\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "        print(f\"Using CPU for comparison\")\n",
    "    \n",
    "    print(\"\\nModel Performance Comparison\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    try:\n",
    "        # Load base model\n",
    "        print(f\"Loading base model: {base_model_name}\")\n",
    "        base_tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "        base_model = PhysBERTEmbeddingModel(base_model_name).to(device)\n",
    "        base_model.eval()\n",
    "        \n",
    "        # Load fine-tuned model\n",
    "        print(f\"Loading fine-tuned model: {finetuned_model_path}\")\n",
    "        finetuned_model, finetuned_tokenizer, _ = load_finetuned_model(finetuned_model_path, device)\n",
    "        finetuned_model.eval()\n",
    "        \n",
    "        # Memory optimization: Only limit if really necessary for MPS\n",
    "        original_num_samples = num_samples\n",
    "        if device.type == 'mps' and num_samples > 10:\n",
    "            num_samples = min(num_samples, 10)  # More reasonable limit for MPS\n",
    "            print(f\"MPS optimization: limited to {num_samples} samples (from {original_num_samples})\")\n",
    "        \n",
    "        # Select sample queries for comparison\n",
    "        sample_triplets = test_triplets[:num_samples]\n",
    "        \n",
    "        print(f\"\\nTesting {len(sample_triplets)} triplets on {device}...\")\n",
    "        \n",
    "        base_improvements = 0\n",
    "        total_tests = 0\n",
    "        detailed_results = []  # Store detailed results for JSON\n",
    "        \n",
    "        with torch.no_grad():  # Disable gradients for entire comparison\n",
    "            for i, triplet in enumerate(sample_triplets, 1):\n",
    "                query = triplet['anchor']\n",
    "                positive = triplet['positive']\n",
    "                negative = triplet['negative']\n",
    "                \n",
    "                print(f\"\\nTest Case {i}:\")\n",
    "                print(f\"Query: {query[:80]}...\")\n",
    "                print(f\"Positive: {positive[:60]}...\")\n",
    "                print(f\"Negative: {negative[:60]}...\")\n",
    "                \n",
    "                # Memory-efficient encoding with smaller batch size\n",
    "                encode_batch_size = 1 if device.type == 'mps' else 4\n",
    "                \n",
    "                # Base model similarities\n",
    "                base_query_emb = base_model.encode([query], base_tokenizer, device, batch_size=encode_batch_size)\n",
    "                base_pos_emb = base_model.encode([positive], base_tokenizer, device, batch_size=encode_batch_size)\n",
    "                base_neg_emb = base_model.encode([negative], base_tokenizer, device, batch_size=encode_batch_size)\n",
    "                \n",
    "                base_pos_sim = cosine_similarity(base_query_emb, base_pos_emb)[0][0]\n",
    "                base_neg_sim = cosine_similarity(base_query_emb, base_neg_emb)[0][0]\n",
    "                base_margin = base_pos_sim - base_neg_sim\n",
    "                \n",
    "                # Clear base model embeddings to free memory\n",
    "                del base_query_emb, base_pos_emb, base_neg_emb\n",
    "                \n",
    "                # Memory cleanup between models\n",
    "                if device.type == 'mps':\n",
    "                    torch.mps.empty_cache()\n",
    "                elif device.type == 'cuda':\n",
    "                    torch.cuda.empty_cache()\n",
    "                \n",
    "                # Fine-tuned model similarities  \n",
    "                ft_query_emb = finetuned_model.encode([query], finetuned_tokenizer, device, batch_size=encode_batch_size)\n",
    "                ft_pos_emb = finetuned_model.encode([positive], finetuned_tokenizer, device, batch_size=encode_batch_size)\n",
    "                ft_neg_emb = finetuned_model.encode([negative], finetuned_tokenizer, device, batch_size=encode_batch_size)\n",
    "                \n",
    "                ft_pos_sim = cosine_similarity(ft_query_emb, ft_pos_emb)[0][0]\n",
    "                ft_neg_sim = cosine_similarity(ft_query_emb, ft_neg_emb)[0][0]\n",
    "                ft_margin = ft_pos_sim - ft_neg_sim\n",
    "                \n",
    "                # Clear fine-tuned model embeddings\n",
    "                del ft_query_emb, ft_pos_emb, ft_neg_emb\n",
    "                \n",
    "                print(f\"\\nResults:\")\n",
    "                print(f\"Base Model     - Pos: {base_pos_sim:.3f}, Neg: {base_neg_sim:.3f}, Margin: {base_margin:.3f}\")\n",
    "                print(f\"Fine-tuned     - Pos: {ft_pos_sim:.3f}, Neg: {ft_neg_sim:.3f}, Margin: {ft_margin:.3f}\")\n",
    "                \n",
    "                # Determine improvement\n",
    "                improvement = ft_margin - base_margin\n",
    "                is_improvement = improvement > 0\n",
    "                \n",
    "                if is_improvement:\n",
    "                    print(f\"Improvement: +{improvement:.3f} (Better separation)\")\n",
    "                    base_improvements += 1\n",
    "                else:\n",
    "                    print(f\"Regression: {improvement:.3f} (Worse separation)\")\n",
    "                \n",
    "                # Store detailed results for JSON\n",
    "                test_result = {\n",
    "                    \"test_case\": i,\n",
    "                    \"triplet_id\": triplet.get('id', f'test_{i}'),\n",
    "                    \"query\": query,\n",
    "                    \"positive\": positive,\n",
    "                    \"negative\": negative,\n",
    "                    \"base_model\": {\n",
    "                        \"positive_similarity\": float(base_pos_sim),\n",
    "                        \"negative_similarity\": float(base_neg_sim),\n",
    "                        \"margin\": float(base_margin)\n",
    "                    },\n",
    "                    \"finetuned_model\": {\n",
    "                        \"positive_similarity\": float(ft_pos_sim),\n",
    "                        \"negative_similarity\": float(ft_neg_sim),\n",
    "                        \"margin\": float(ft_margin)\n",
    "                    },\n",
    "                    \"comparison\": {\n",
    "                        \"improvement\": float(improvement),\n",
    "                        \"is_improvement\": is_improvement,\n",
    "                        \"improvement_percentage\": float((improvement / abs(base_margin)) * 100) if base_margin != 0 else 0.0\n",
    "                    }\n",
    "                }\n",
    "                detailed_results.append(test_result)\n",
    "                \n",
    "                total_tests += 1\n",
    "                \n",
    "                # Memory cleanup after each test\n",
    "                if device.type == 'mps':\n",
    "                    torch.mps.empty_cache()\n",
    "                elif device.type == 'cuda':\n",
    "                    torch.cuda.empty_cache()\n",
    "                \n",
    "                print(\"-\" * 40)\n",
    "        \n",
    "        # Clear models from memory\n",
    "        del base_model, finetuned_model\n",
    "        if device.type == 'mps':\n",
    "            torch.mps.empty_cache()\n",
    "        elif device.type == 'cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        # Summary\n",
    "        improvement_rate = (base_improvements / total_tests) * 100\n",
    "        print(f\"\\nSummary:\")\n",
    "        print(f\"  Tests with improvement: {base_improvements}/{total_tests} ({improvement_rate:.1f}%)\")\n",
    "        print(f\"  Tests with regression: {total_tests - base_improvements}/{total_tests} ({100 - improvement_rate:.1f}%)\")\n",
    "        print(f\"  Device used: {device}\")\n",
    "        print(f\"  Memory optimization: {'ON' if device.type == 'mps' else 'OFF'}\")\n",
    "        \n",
    "        # Prepare complete results dictionary\n",
    "        results_dict = {\n",
    "            'total_tests': total_tests,\n",
    "            'improvements': base_improvements,\n",
    "            'improvement_rate': improvement_rate,\n",
    "            'device': str(device),\n",
    "            'memory_optimized': device.type == 'mps'\n",
    "        }\n",
    "        \n",
    "        # Save detailed results to JSON if requested\n",
    "        if save_results:\n",
    "            complete_results = {\n",
    "                \"metadata\": {\n",
    "                    \"timestamp\": datetime.now().isoformat(),\n",
    "                    \"base_model\": base_model_name,\n",
    "                    \"finetuned_model\": finetuned_model_path,\n",
    "                    \"device\": str(device),\n",
    "                    \"memory_optimized\": device.type == 'mps',\n",
    "                    \"total_tests\": total_tests,\n",
    "                    \"improvements\": base_improvements,\n",
    "                    \"improvement_rate\": improvement_rate\n",
    "                },\n",
    "                \"test_results\": detailed_results,\n",
    "                \"summary\": {\n",
    "                    \"average_base_positive_similarity\": sum([r[\"base_model\"][\"positive_similarity\"] for r in detailed_results]) / len(detailed_results),\n",
    "                    \"average_base_negative_similarity\": sum([r[\"base_model\"][\"negative_similarity\"] for r in detailed_results]) / len(detailed_results),\n",
    "                    \"average_base_margin\": sum([r[\"base_model\"][\"margin\"] for r in detailed_results]) / len(detailed_results),\n",
    "                    \"average_finetuned_positive_similarity\": sum([r[\"finetuned_model\"][\"positive_similarity\"] for r in detailed_results]) / len(detailed_results),\n",
    "                    \"average_finetuned_negative_similarity\": sum([r[\"finetuned_model\"][\"negative_similarity\"] for r in detailed_results]) / len(detailed_results),\n",
    "                    \"average_finetuned_margin\": sum([r[\"finetuned_model\"][\"margin\"] for r in detailed_results]) / len(detailed_results),\n",
    "                    \"average_improvement\": sum([r[\"comparison\"][\"improvement\"] for r in detailed_results]) / len(detailed_results)\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            # Save to JSON file\n",
    "            try:\n",
    "                with open(results_file, 'w', encoding='utf-8') as f:\n",
    "                    json.dump(complete_results, f, indent=2, ensure_ascii=False)\n",
    "                print(f\"\\nDetailed results saved to: {results_file}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Failed to save results to JSON: {e}\")\n",
    "        \n",
    "        return results_dict\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Comparison failed: {e}\")\n",
    "        # Emergency memory cleanup\n",
    "        if device.type == 'mps':\n",
    "            torch.mps.empty_cache()\n",
    "        elif device.type == 'cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dae4c61c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PhysBERT Transformers Fine-tuning Pipeline\n",
      "==================================================\n",
      "\n",
      "Memory Optimization Setup\n",
      "ðŸ”§ Applied memory optimizations:\n",
      "- MPS cache cleared\n",
      "- Garbage collection completed\n",
      "ðŸŽ MPS Memory Status:\n",
      "   - MPS Available: True\n",
      "   - MPS Built: True\n",
      "Memory cache cleared\n",
      "\n",
      "Step 1: Loading and Splitting Training Data\n",
      "Loaded 53 triplet examples for training\n",
      "Data split:\n",
      "  - Training examples: 48\n",
      "  - Test examples: 5 (held out for evaluation)\n",
      "  - Total examples: 53\n",
      "\n",
      "Test Set (Held Out for Evaluation):\n",
      "  Test 1: Query: 'What is a ``minimum-ionizing particle\" (mip)?...'\n",
      "  Test 2: Query: 'What is the photon mass attenuation length?...'\n",
      "  Test 3: Query: 'What is the formula for plasma energy, _{p}?...'\n",
      "  Test 4: Query: 'How can you calculate the radiation length for a c...'\n",
      "  Test 5: Query: 'What is the formula for the collision stopping pow...'\n",
      "\n",
      "Step 2: Fine-tuning PhysBERT Model (Training Set Only)\n",
      "PhysBERT Transformers Fine-tuning Pipeline\n",
      "==================================================\n",
      "Using Apple Silicon MPS: mps\n",
      "Loading base model: thellert/physbert_cased\n",
      "Enabled gradient checkpointing\n",
      "Model and tokenizer loaded successfully\n",
      "Loaded 48 triplet examples for training\n",
      "Using 48 examples for training\n",
      "Data augmentation: 48 -> 192 examples\n",
      "MPS optimization: batch size reduced from 8 to 4\n",
      "ðŸš€ Training Configuration:\n",
      "   - Examples: 192\n",
      "   - Batch size: 4 (gradient accumulation: 2)\n",
      "   - Epochs: 3\n",
      "   - Learning rate: 2e-05\n",
      "   - Total steps: 144\n",
      "   - Warmup steps: 14\n",
      "   - Max length: 512\n",
      "   - Device: mps\n",
      "\n",
      "Starting training...\n",
      "\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/48 [00:00<?, ?it/s]/Users/sandeshs/Documents/Projects/LLMTest/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Epoch 1:   2%|â–         | 1/48 [00:03<02:23,  3.04s/it, loss=0.3667, avg_loss=0.0000, lr=0.00e+00, device=mps, mem_opt=ON]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 0 similarities: {'positive_similarity': 0.21358193457126617, 'negative_similarity': 0.08025713264942169, 'margin': 0.13332481682300568}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [01:53<00:00,  2.37s/it, loss=0.1247, avg_loss=0.2346, lr=1.85e-05, device=mps, mem_opt=ON]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 average loss: 0.2346\n",
      "\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [01:50<00:00,  2.30s/it, loss=0.0000, avg_loss=0.0455, lr=1.48e-05, device=mps, mem_opt=ON]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 average loss: 0.0455\n",
      "\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:   8%|â–Š         | 4/48 [00:09<01:41,  2.31s/it, loss=0.0000, avg_loss=0.0000, lr=1.45e-05, device=mps, mem_opt=ON]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 50 similarities: {'positive_similarity': 0.7070565223693848, 'negative_similarity': -0.15934453904628754, 'margin': 0.8664010763168335}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  10%|â–ˆ         | 5/48 [00:11<01:38,  2.30s/it, loss=0.0528, avg_loss=0.0000, lr=1.45e-05, device=mps, mem_opt=ON]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 50 similarities: {'positive_similarity': 0.6735079884529114, 'negative_similarity': 0.043615713715553284, 'margin': 0.6298922300338745}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [01:49<00:00,  2.29s/it, loss=0.0000, avg_loss=0.0148, lr=1.11e-05, device=mps, mem_opt=ON]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 average loss: 0.0148\n",
      "\n",
      "Training completed. Average loss: 0.0983\n",
      "Model saved to: models/physbert-physics-finetuned\n",
      "Training metadata saved to models/physbert-physics-finetuned/training_metadata_transformers.yaml\n",
      "\n",
      "Step 3: Comparing Model Performance on Held-Out Test Set\n",
      "Testing on 5 previously unseen examples...\n",
      "Using Apple Silicon MPS for comparison\n",
      "\n",
      "Model Performance Comparison\n",
      "========================================\n",
      "Loading base model: thellert/physbert_cased\n",
      "Loading fine-tuned model: models/physbert-physics-finetuned\n",
      "Loaded fine-tuned model from models/physbert-physics-finetuned on mps\n",
      "\n",
      "Testing 5 triplets on mps...\n",
      "\n",
      "Test Case 1:\n",
      "Query: What is a ``minimum-ionizing particle\" (mip)?...\n",
      "Positive: The stopping power functions are characterized by broad mini...\n",
      "Negative: For protons of less than several hundred eV, non-ionizing nu...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sandeshs/Documents/Projects/LLMTest/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "/Users/sandeshs/Documents/Projects/LLMTest/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results:\n",
      "Base Model     - Pos: 0.262, Neg: 0.242, Margin: 0.020\n",
      "Fine-tuned     - Pos: 0.449, Neg: 0.178, Margin: 0.270\n",
      "Improvement: +0.250 (Better separation)\n",
      "----------------------------------------\n",
      "\n",
      "Test Case 2:\n",
      "Query: What is the photon mass attenuation length?...\n",
      "Positive: The photon mass attenuation length (or mean free path) is =1...\n",
      "Negative: The characteristic amount of matter traversed for [bremsstra...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sandeshs/Documents/Projects/LLMTest/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "/Users/sandeshs/Documents/Projects/LLMTest/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results:\n",
      "Base Model     - Pos: 0.336, Neg: 0.191, Margin: 0.145\n",
      "Fine-tuned     - Pos: 0.796, Neg: 0.487, Margin: 0.309\n",
      "Improvement: +0.164 (Better separation)\n",
      "----------------------------------------\n",
      "\n",
      "Test Case 3:\n",
      "Query: What is the formula for plasma energy, _{p}?...\n",
      "Positive: The plasma energy is _{p} = 28.816~eV. _{p}: The plasma ener...\n",
      "Negative: The determination of the mean excitation energy is the princ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sandeshs/Documents/Projects/LLMTest/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "/Users/sandeshs/Documents/Projects/LLMTest/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results:\n",
      "Base Model     - Pos: 0.203, Neg: 0.103, Margin: 0.101\n",
      "Fine-tuned     - Pos: 0.664, Neg: 0.126, Margin: 0.538\n",
      "Improvement: +0.438 (Better separation)\n",
      "----------------------------------------\n",
      "\n",
      "Test Case 4:\n",
      "Query: How can you calculate the radiation length for a chemical compound?...\n",
      "Positive: The radiation length in a mixture or compound may be approxi...\n",
      "Negative: A mixture or compound can be thought of as made up of thin l...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sandeshs/Documents/Projects/LLMTest/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "/Users/sandeshs/Documents/Projects/LLMTest/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results:\n",
      "Base Model     - Pos: 0.153, Neg: 0.123, Margin: 0.030\n",
      "Fine-tuned     - Pos: 0.681, Neg: 0.289, Margin: 0.392\n",
      "Improvement: +0.362 (Better separation)\n",
      "----------------------------------------\n",
      "\n",
      "Test Case 5:\n",
      "Query: What is the formula for the collision stopping power for positrons?...\n",
      "Positive: Electron-positron scattering is described by the Bhabha cros...\n",
      "Negative: For electrons, large energy transfers to atomic electrons ar...\n",
      "\n",
      "Results:\n",
      "Base Model     - Pos: 0.388, Neg: 0.310, Margin: 0.078\n",
      "Fine-tuned     - Pos: 0.552, Neg: 0.331, Margin: 0.222\n",
      "Improvement: +0.143 (Better separation)\n",
      "----------------------------------------\n",
      "\n",
      "Summary:\n",
      "  Tests with improvement: 5/5 (100.0%)\n",
      "  Tests with regression: 0/5 (0.0%)\n",
      "  Device used: mps\n",
      "  Memory optimization: ON\n",
      "Warning: Failed to save results to JSON: Object of type bool is not JSON serializable\n",
      "\n",
      "Pipeline completed successfully!\n",
      "Fine-tuned model saved to: models/physbert-physics-finetuned\n",
      "\n",
      "Evaluation Results (Held-Out Test Set):\n",
      "  - Test examples: 5\n",
      "  - Improvements: 5\n",
      "  - Improvement rate: 100.0%\n",
      "  - Training examples: 48\n",
      "  - Memory optimizations applied for Apple Silicon\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sandeshs/Documents/Projects/LLMTest/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "def run_complete_transformers_pipeline():\n",
    "    \"\"\"\n",
    "    Complete pipeline: Train transformers model and compare performance\n",
    "    Following RAGAS framework patterns with memory optimization and proper train/test split\n",
    "    \"\"\"\n",
    "    print(\"PhysBERT Transformers Fine-tuning Pipeline\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Memory optimization setup\n",
    "    print(\"\\nMemory Optimization Setup\")\n",
    "    optimize_for_memory()\n",
    "    check_mps_memory()\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Load and prepare training data with proper train/test split\n",
    "        print(\"\\nStep 1: Loading and Splitting Training Data\")\n",
    "        config_path = 'config/physbert_config.yaml'\n",
    "        all_triplets = load_triplets_from_json('data/physics_triplets_st.json')\n",
    "        \n",
    "        if not all_triplets:\n",
    "            print(\"No training data found. Please check data path.\")\n",
    "            return\n",
    "        \n",
    "        # # Apply demo size limit if specified\n",
    "        # demo_size = int(os.getenv('PHYSBERT_TEST_SIZE', len(all_triplets)))\n",
    "        # if demo_size < len(all_triplets):\n",
    "        #     all_triplets = all_triplets[:demo_size]\n",
    "        #     print(f\"Limited to {len(all_triplets)} examples for demonstration\")\n",
    "        \n",
    "        # Create proper train/test split: Hold out 5 random triplets for testing\n",
    "        test_size = min(5, len(all_triplets) // 4)  # Use 5 or 25% of data, whichever is smaller\n",
    "        \n",
    "        # Randomly shuffle and split the data\n",
    "        random.seed(42)  # Set seed for reproducible results\n",
    "        shuffled_triplets = all_triplets.copy()\n",
    "        random.shuffle(shuffled_triplets)\n",
    "        \n",
    "        # Split into test and train sets\n",
    "        test_triplets = shuffled_triplets[:test_size]\n",
    "        train_triplets = shuffled_triplets[test_size:]\n",
    "        \n",
    "        print(f\"Data split:\")\n",
    "        print(f\"  - Training examples: {len(train_triplets)}\")\n",
    "        print(f\"  - Test examples: {len(test_triplets)} (held out for evaluation)\")\n",
    "        print(f\"  - Total examples: {len(all_triplets)}\")\n",
    "        \n",
    "        # Display test set examples for transparency\n",
    "        print(f\"\\nTest Set (Held Out for Evaluation):\")\n",
    "        for i, triplet in enumerate(test_triplets, 1):\n",
    "            print(f\"  Test {i}: Query: '{triplet['anchor'][:50]}...'\")\n",
    "        \n",
    "        # Memory check before training\n",
    "        if torch.backends.mps.is_available():\n",
    "            torch.mps.empty_cache()\n",
    "        \n",
    "        # Step 2: Train the fine-tuned model on training set only\n",
    "        print(\"\\nStep 2: Fine-tuning PhysBERT Model (Training Set Only)\")\n",
    "        \n",
    "        # Temporarily modify the data loading function to use our train split\n",
    "        original_triplets_path = 'data/physics_triplets_st.json'\n",
    "        train_triplets_path = 'data/physics_triplets_train_split.json'\n",
    "        \n",
    "        # Save training split to temporary file\n",
    "        os.makedirs('data', exist_ok=True)\n",
    "        train_data = []\n",
    "        for triplet in train_triplets:\n",
    "            train_data.append({\n",
    "                'texts': [triplet['anchor'], triplet['positive'], triplet['negative']],\n",
    "                'id': triplet['id']\n",
    "            })\n",
    "        \n",
    "        with open(train_triplets_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(train_data, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        # Set environment variable to use training split\n",
    "        os.environ['TRIPLET_DATA_PATH'] = train_triplets_path\n",
    "        \n",
    "        try:\n",
    "            trained_model, tokenizer, config = train_physbert_transformers(config_path)\n",
    "        finally:\n",
    "            # Restore original data path and cleanup temp file\n",
    "            if 'TRIPLET_DATA_PATH' in os.environ:\n",
    "                del os.environ['TRIPLET_DATA_PATH']\n",
    "            if os.path.exists(train_triplets_path):\n",
    "                os.remove(train_triplets_path)\n",
    "        \n",
    "        if not trained_model:\n",
    "            print(\"Training failed\")\n",
    "            return\n",
    "        \n",
    "        # Memory cleanup between training and evaluation\n",
    "        if torch.backends.mps.is_available():\n",
    "            torch.mps.empty_cache()\n",
    "        \n",
    "        # Step 3: Compare model performances on held-out test set\n",
    "        print(\"\\nStep 3: Comparing Model Performance on Held-Out Test Set\")\n",
    "        print(f\"Testing on {len(test_triplets)} previously unseen examples...\")\n",
    "        \n",
    "        base_model_name = config.get('model', {}).get('base_model', 'thellert/physbert_cased')\n",
    "        finetuned_model_path = config.get('training', {}).get('output_path', 'models/physbert-transformers-finetuned')\n",
    "        \n",
    "        # Test on the held-out test set (not used during training)\n",
    "        comparison_results = compare_models_performance_transformers(\n",
    "            base_model_name, \n",
    "            finetuned_model_path, \n",
    "            test_triplets,  # Use held-out test set\n",
    "            num_samples=len(test_triplets)  # Test all held-out examples\n",
    "        )\n",
    "        \n",
    "        # Final memory cleanup\n",
    "        if torch.backends.mps.is_available():\n",
    "            torch.mps.empty_cache()\n",
    "        \n",
    "        print(\"\\nPipeline completed successfully!\")\n",
    "        print(f\"Fine-tuned model saved to: {finetuned_model_path}\")\n",
    "        print(f\"\\nEvaluation Results (Held-Out Test Set):\")\n",
    "        print(f\"  - Test examples: {comparison_results['total_tests']}\")\n",
    "        print(f\"  - Improvements: {comparison_results['improvements']}\")\n",
    "        print(f\"  - Improvement rate: {comparison_results['improvement_rate']:.1f}%\")\n",
    "        print(f\"  - Training examples: {len(train_triplets)}\")\n",
    "        \n",
    "        if comparison_results.get('memory_optimized'):\n",
    "            print(\"  - Memory optimizations applied for Apple Silicon\")\n",
    "        \n",
    "        # Enhanced results with train/test split info\n",
    "        results = {\n",
    "            'trained_model': trained_model,\n",
    "            'tokenizer': tokenizer,\n",
    "            'config': config,\n",
    "            'training_examples': len(train_triplets),\n",
    "            'test_examples': len(test_triplets),\n",
    "            'total_examples': len(all_triplets),\n",
    "            'test_triplets': test_triplets,  # Include test set for further analysis\n",
    "            'comparison_results': comparison_results,\n",
    "            'memory_optimized': comparison_results.get('memory_optimized', False),\n",
    "            'train_test_split': {\n",
    "                'train_size': len(train_triplets),\n",
    "                'test_size': len(test_triplets),\n",
    "                'split_ratio': f\"{len(train_triplets)}:{len(test_triplets)}\"\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Pipeline failed: {e}\")\n",
    "        # Emergency memory cleanup\n",
    "        if torch.backends.mps.is_available():\n",
    "            torch.mps.empty_cache()\n",
    "        raise\n",
    "\n",
    "# Execute the complete pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    results = run_complete_transformers_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d47b3ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_test_results(results_dict):\n",
    "    \"\"\"\n",
    "    Provide detailed analysis of the test results from the pipeline\n",
    "    \n",
    "    Args:\n",
    "        results_dict: Results dictionary returned from run_complete_transformers_pipeline()\n",
    "    \"\"\"\n",
    "    if not results_dict or 'comparison_results' not in results_dict:\n",
    "        print(\"No valid results to analyze\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\nDetailed Test Results Analysis\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Extract key metrics\n",
    "    comparison = results_dict['comparison_results']\n",
    "    train_test_info = results_dict.get('train_test_split', {})\n",
    "    \n",
    "    print(f\"Dataset Split Information:\")\n",
    "    print(f\"  - Total examples loaded: {results_dict.get('total_examples', 'N/A')}\")\n",
    "    print(f\"  - Training examples: {results_dict.get('training_examples', 'N/A')}\")\n",
    "    print(f\"  - Test examples: {results_dict.get('test_examples', 'N/A')}\")\n",
    "    print(f\"  - Split ratio: {train_test_info.get('split_ratio', 'N/A')}\")\n",
    "    \n",
    "    print(f\"\\nModel Evaluation on Held-Out Test Set:\")\n",
    "    print(f\"  - Test cases evaluated: {comparison.get('total_tests', 0)}\")\n",
    "    print(f\"  - Cases with improvement: {comparison.get('improvements', 0)}\")\n",
    "    print(f\"  - Cases with regression: {comparison.get('total_tests', 0) - comparison.get('improvements', 0)}\")\n",
    "    print(f\"  - Overall improvement rate: {comparison.get('improvement_rate', 0):.1f}%\")\n",
    "    \n",
    "    # Performance interpretation\n",
    "    improvement_rate = comparison.get('improvement_rate', 0)\n",
    "    if improvement_rate >= 80:\n",
    "        performance_level = \"Excellent\"\n",
    "        interpretation = \"The fine-tuning significantly improved the model's ability to distinguish relevant physics content.\"\n",
    "    elif improvement_rate >= 60:\n",
    "        performance_level = \"Good\"\n",
    "        interpretation = \"The fine-tuning showed substantial improvements in most test cases.\"\n",
    "    elif improvement_rate >= 40:\n",
    "        performance_level = \"Moderate\"\n",
    "        interpretation = \"The fine-tuning showed some improvements but may need more training data or epochs.\"\n",
    "    elif improvement_rate >= 20:\n",
    "        performance_level = \"Limited\"\n",
    "        interpretation = \"The fine-tuning showed minimal improvements. Consider adjusting hyperparameters.\"\n",
    "    else:\n",
    "        performance_level = \"Poor\"\n",
    "        interpretation = \"The fine-tuning did not improve performance. Review training data and approach.\"\n",
    "    \n",
    "    print(f\"\\nPerformance Assessment:\")\n",
    "    print(f\"  - Performance Level: {performance_level}\")\n",
    "    print(f\"  - Interpretation: {interpretation}\")\n",
    "    \n",
    "    # Technical details\n",
    "    print(f\"\\nTechnical Details:\")\n",
    "    print(f\"  - Device used: {comparison.get('device', 'N/A')}\")\n",
    "    print(f\"  - Memory optimization: {'Enabled' if comparison.get('memory_optimized') else 'Disabled'}\")\n",
    "    \n",
    "    # Test set transparency\n",
    "    if 'test_triplets' in results_dict and results_dict['test_triplets']:\n",
    "        print(f\"\\nTest Set Examples (for transparency):\")\n",
    "        for i, triplet in enumerate(results_dict['test_triplets'], 1):\n",
    "            print(f\"  Test {i}:\")\n",
    "            print(f\"    Query: '{triplet['anchor'][:60]}...'\")\n",
    "            print(f\"    Positive: '{triplet['positive'][:60]}...'\")\n",
    "            print(f\"    Negative: '{triplet['negative'][:60]}...'\")\n",
    "    \n",
    "    # Recommendations\n",
    "    print(f\"\\nRecommendations:\")\n",
    "    if improvement_rate >= 60:\n",
    "        print(\"  âœ… Model is ready for physics domain tasks\")\n",
    "        print(\"  âœ… Consider testing on more diverse physics content\")\n",
    "    elif improvement_rate >= 40:\n",
    "        print(\"  âš ï¸  Consider increasing training epochs or data augmentation\")\n",
    "        print(\"  âš ï¸  Review learning rate and batch size settings\")\n",
    "    else:\n",
    "        print(\"  âŒ Review training data quality and relevance\")\n",
    "        print(\"  âŒ Consider different model architecture or loss function\")\n",
    "        print(\"  âŒ Increase training data size or diversity\")\n",
    "    \n",
    "    return {\n",
    "        'performance_level': performance_level,\n",
    "        'improvement_rate': improvement_rate,\n",
    "        'interpretation': interpretation,\n",
    "        'total_tests': comparison.get('total_tests', 0),\n",
    "        'improvements': comparison.get('improvements', 0)\n",
    "    }\n",
    "\n",
    "\n",
    "def run_evaluation_only(model_path: str = \"/Users/sandeshs/Documents/Projects/LLMTest/tests/ouragboros/models/physbert-physics-finetuned\",\n",
    "                       test_data_path: str = \"data/physics_triplets_st.json\",\n",
    "                       num_test_samples: int = 5,\n",
    "                       save_json: bool = True,\n",
    "                       results_filename: str = None):\n",
    "    \"\"\"\n",
    "    Run evaluation only (without training) on a random test set\n",
    "    Useful for testing an already trained model\n",
    "    \n",
    "    Args:\n",
    "        model_path: Path to the fine-tuned model\n",
    "        test_data_path: Path to the test data\n",
    "        num_test_samples: Number of random samples to test\n",
    "        save_json: Whether to save detailed results as JSON\n",
    "        results_filename: Custom filename for results (default: auto-generated)\n",
    "        \n",
    "    Returns:\n",
    "        Comparison results dictionary\n",
    "    \"\"\"\n",
    "    print(\"Model Evaluation Pipeline (No Training)\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    try:\n",
    "        # Load all available triplets\n",
    "        all_triplets = load_triplets_from_json(test_data_path)\n",
    "        \n",
    "        if not all_triplets:\n",
    "            print(\"No test data found.\")\n",
    "            return None\n",
    "        \n",
    "        # Create random test set\n",
    "        random.seed(42)  # Reproducible results\n",
    "        test_triplets = random.sample(all_triplets, min(num_test_samples, len(all_triplets)))\n",
    "        \n",
    "        print(f\"Selected {len(test_triplets)} random test examples\")\n",
    "        \n",
    "        # Generate results filename if not provided\n",
    "        if results_filename is None:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            model_name = os.path.basename(model_path).replace(\"-\", \"_\")\n",
    "            results_filename = f\"physbert_evaluation_{model_name}_{timestamp}.json\"\n",
    "        \n",
    "        # Run comparison with JSON saving\n",
    "        base_model_name = \"thellert/physbert_cased\"\n",
    "        comparison_results = compare_models_performance_transformers(\n",
    "            base_model_name,\n",
    "            model_path,\n",
    "            test_triplets,\n",
    "            num_samples=len(test_triplets),\n",
    "            save_results=save_json,\n",
    "            results_file=results_filename\n",
    "        )\n",
    "        \n",
    "        # Analyze results\n",
    "        eval_results = {\n",
    "            'comparison_results': comparison_results,\n",
    "            'test_examples': len(test_triplets),\n",
    "            'test_triplets': test_triplets,\n",
    "            'total_examples': len(all_triplets),\n",
    "            'results_file': results_filename if save_json else None\n",
    "        }\n",
    "        \n",
    "        analysis = analyze_test_results(eval_results)\n",
    "        \n",
    "        if save_json:\n",
    "            print(f\"\\nðŸ“Š Detailed test results with individual scores saved to: {results_filename}\")\n",
    "            print(\"   The JSON file contains:\")\n",
    "            print(\"   - Individual test case scores for both models\")\n",
    "            print(\"   - Similarity scores (positive, negative, margin)\")\n",
    "            print(\"   - Improvement analysis for each test\")\n",
    "            print(\"   - Summary statistics and averages\")\n",
    "        \n",
    "        return eval_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Evaluation failed: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def load_and_analyze_json_results(json_file: str):\n",
    "    \"\"\"\n",
    "    Load and analyze previously saved JSON results\n",
    "    \n",
    "    Args:\n",
    "        json_file: Path to the JSON results file\n",
    "        \n",
    "    Returns:\n",
    "        Loaded results dictionary\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(json_file, 'r', encoding='utf-8') as f:\n",
    "            results = json.load(f)\n",
    "        \n",
    "        print(f\"ðŸ“ˆ Analysis of results from: {json_file}\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        metadata = results.get('metadata', {})\n",
    "        test_results = results.get('test_results', [])\n",
    "        summary = results.get('summary', {})\n",
    "        \n",
    "        print(f\"Evaluation Metadata:\")\n",
    "        print(f\"  - Timestamp: {metadata.get('timestamp', 'N/A')}\")\n",
    "        print(f\"  - Base Model: {metadata.get('base_model', 'N/A')}\")\n",
    "        print(f\"  - Fine-tuned Model: {os.path.basename(metadata.get('finetuned_model', 'N/A'))}\")\n",
    "        print(f\"  - Device: {metadata.get('device', 'N/A')}\")\n",
    "        print(f\"  - Total Tests: {metadata.get('total_tests', 0)}\")\n",
    "        print(f\"  - Improvement Rate: {metadata.get('improvement_rate', 0):.1f}%\")\n",
    "        \n",
    "        print(f\"\\nPer-Test Case Analysis:\")\n",
    "        for i, test in enumerate(test_results, 1):\n",
    "            base = test['base_model']\n",
    "            ft = test['finetuned_model']\n",
    "            comp = test['comparison']\n",
    "            \n",
    "            print(f\"  Test {i} (ID: {test.get('triplet_id', 'N/A')}):\")\n",
    "            print(f\"    Query: '{test['query'][:50]}...'\")\n",
    "            print(f\"    Base Model    - Pos: {base['positive_similarity']:.3f}, Neg: {base['negative_similarity']:.3f}, Margin: {base['margin']:.3f}\")\n",
    "            print(f\"    Fine-tuned    - Pos: {ft['positive_similarity']:.3f}, Neg: {ft['negative_similarity']:.3f}, Margin: {ft['margin']:.3f}\")\n",
    "            print(f\"    Improvement   - {comp['improvement']:+.3f} ({'âœ…' if comp['is_improvement'] else 'âŒ'})\")\n",
    "            print()\n",
    "        \n",
    "        print(f\"Summary Averages:\")\n",
    "        print(f\"  - Base Model Average Margin: {summary.get('average_base_margin', 0):.3f}\")\n",
    "        print(f\"  - Fine-tuned Average Margin: {summary.get('average_finetuned_margin', 0):.3f}\")\n",
    "        print(f\"  - Average Improvement: {summary.get('average_improvement', 0):.3f}\")\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"âŒ Results file not found: {json_file}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error loading results: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8202c09f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Evaluation Pipeline (No Training)\n",
      "==================================================\n",
      "Loaded 53 triplet examples for training\n",
      "Selected 5 random test examples\n",
      "Using Apple Silicon MPS for comparison\n",
      "\n",
      "Model Performance Comparison\n",
      "========================================\n",
      "Loading base model: thellert/physbert_cased\n",
      "Loading fine-tuned model: /Users/sandeshs/Documents/Projects/LLMTest/tests/ouragboros/models/physbert-physics-finetuned\n",
      "Loaded fine-tuned model from /Users/sandeshs/Documents/Projects/LLMTest/tests/ouragboros/models/physbert-physics-finetuned on mps\n",
      "\n",
      "Testing 5 triplets on mps...\n",
      "\n",
      "Test Case 1:\n",
      "Query: How does the minimum ionization value change with the atomic number Z of the abs...\n",
      "Positive: Except in hydrogen, particles with the same velocity have si...\n",
      "Negative: Figure 34.14 shows that the electron critical energy for the...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sandeshs/Documents/Projects/LLMTest/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "/Users/sandeshs/Documents/Projects/LLMTest/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results:\n",
      "Base Model     - Pos: 0.179, Neg: 0.257, Margin: -0.078\n",
      "Fine-tuned     - Pos: 0.675, Neg: -0.068, Margin: 0.742\n",
      "Improvement: +0.820 (Better separation)\n",
      "----------------------------------------\n",
      "\n",
      "Test Case 2:\n",
      "Query: How is the longitudinal profile of energy deposition in an electromagnetic casca...\n",
      "Positive: The mean longitudinal profile of the energy deposition in an...\n",
      "Negative: The transverse development of electromagnetic showers in dif...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sandeshs/Documents/Projects/LLMTest/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "/Users/sandeshs/Documents/Projects/LLMTest/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "/Users/sandeshs/Documents/Projects/LLMTest/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results:\n",
      "Base Model     - Pos: 0.407, Neg: 0.275, Margin: 0.132\n",
      "Fine-tuned     - Pos: 0.850, Neg: 0.018, Margin: 0.832\n",
      "Improvement: +0.700 (Better separation)\n",
      "----------------------------------------\n",
      "\n",
      "Test Case 3:\n",
      "Query: What is the formula for the most probable energy loss in a moderately thick dete...\n",
      "Positive: For detectors of moderate thickness, the energy loss probabi...\n",
      "Negative: The mean rate of energy loss by moderately relativistic char...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sandeshs/Documents/Projects/LLMTest/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "/Users/sandeshs/Documents/Projects/LLMTest/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results:\n",
      "Base Model     - Pos: 0.549, Neg: 0.189, Margin: 0.360\n",
      "Fine-tuned     - Pos: 0.846, Neg: -0.183, Margin: 1.028\n",
      "Improvement: +0.668 (Better separation)\n",
      "----------------------------------------\n",
      "\n",
      "Test Case 4:\n",
      "Query: What does the density effect correction, (), account for?...\n",
      "Positive: As the particle energy increases, its electric field flatten...\n",
      "Negative: The Bloch correction z^{2}L_{2} is a low-energy correction t...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sandeshs/Documents/Projects/LLMTest/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "/Users/sandeshs/Documents/Projects/LLMTest/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results:\n",
      "Base Model     - Pos: -0.010, Neg: -0.010, Margin: -0.001\n",
      "Fine-tuned     - Pos: 0.592, Neg: -0.140, Margin: 0.732\n",
      "Improvement: +0.733 (Better separation)\n",
      "----------------------------------------\n",
      "\n",
      "Test Case 5:\n",
      "Query: What is ``stopping power\"?...\n",
      "Positive: The mean rate of energy loss by moderately relativistic char...\n",
      "Negative: Eq. (34.5) may be integrated to find the total (or partial) ...\n",
      "\n",
      "Results:\n",
      "Base Model     - Pos: 0.225, Neg: 0.154, Margin: 0.071\n",
      "Fine-tuned     - Pos: 0.716, Neg: -0.088, Margin: 0.804\n",
      "Improvement: +0.733 (Better separation)\n",
      "----------------------------------------\n",
      "\n",
      "Summary:\n",
      "  Tests with improvement: 5/5 (100.0%)\n",
      "  Tests with regression: 0/5 (0.0%)\n",
      "  Device used: mps\n",
      "  Memory optimization: ON\n",
      "Warning: Failed to save results to JSON: Object of type bool is not JSON serializable\n",
      "\n",
      "Detailed Test Results Analysis\n",
      "==================================================\n",
      "Dataset Split Information:\n",
      "  - Total examples loaded: 53\n",
      "  - Training examples: N/A\n",
      "  - Test examples: 5\n",
      "  - Split ratio: N/A\n",
      "\n",
      "Model Evaluation on Held-Out Test Set:\n",
      "  - Test cases evaluated: 5\n",
      "  - Cases with improvement: 5\n",
      "  - Cases with regression: 0\n",
      "  - Overall improvement rate: 100.0%\n",
      "\n",
      "Performance Assessment:\n",
      "  - Performance Level: Excellent\n",
      "  - Interpretation: The fine-tuning significantly improved the model's ability to distinguish relevant physics content.\n",
      "\n",
      "Technical Details:\n",
      "  - Device used: mps\n",
      "  - Memory optimization: Enabled\n",
      "\n",
      "Test Set Examples (for transparency):\n",
      "  Test 1:\n",
      "    Query: 'How does the minimum ionization value change with the atomic...'\n",
      "    Positive: 'Except in hydrogen, particles with the same velocity have si...'\n",
      "    Negative: 'Figure 34.14 shows that the electron critical energy for the...'\n",
      "  Test 2:\n",
      "    Query: 'How is the longitudinal profile of energy deposition in an e...'\n",
      "    Positive: 'The mean longitudinal profile of the energy deposition in an...'\n",
      "    Negative: 'The transverse development of electromagnetic showers in dif...'\n",
      "  Test 3:\n",
      "    Query: 'What is the formula for the most probable energy loss in a m...'\n",
      "    Positive: 'For detectors of moderate thickness, the energy loss probabi...'\n",
      "    Negative: 'The mean rate of energy loss by moderately relativistic char...'\n",
      "  Test 4:\n",
      "    Query: 'What does the density effect correction, (), account for?...'\n",
      "    Positive: 'As the particle energy increases, its electric field flatten...'\n",
      "    Negative: 'The Bloch correction z^{2}L_{2} is a low-energy correction t...'\n",
      "  Test 5:\n",
      "    Query: 'What is ``stopping power\"?...'\n",
      "    Positive: 'The mean rate of energy loss by moderately relativistic char...'\n",
      "    Negative: 'Eq. (34.5) may be integrated to find the total (or partial) ...'\n",
      "\n",
      "Recommendations:\n",
      "  âœ… Model is ready for physics domain tasks\n",
      "  âœ… Consider testing on more diverse physics content\n",
      "\n",
      "ðŸ“Š Detailed test results with individual scores saved to: physbert_evaluation_physbert_physics_finetuned_20250910_132516.json\n",
      "   The JSON file contains:\n",
      "   - Individual test case scores for both models\n",
      "   - Similarity scores (positive, negative, margin)\n",
      "   - Improvement analysis for each test\n",
      "   - Summary statistics and averages\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sandeshs/Documents/Projects/LLMTest/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'comparison_results': {'total_tests': 5,\n",
       "  'improvements': 5,\n",
       "  'improvement_rate': 100.0,\n",
       "  'device': 'mps',\n",
       "  'memory_optimized': True},\n",
       " 'test_examples': 5,\n",
       " 'test_triplets': [{'anchor': 'How does the minimum ionization value change with the atomic number Z of the absorber?',\n",
       "   'positive': 'Except in hydrogen, particles with the same velocity have similar rates of energy loss in different materials, although there is a slow decrease in the rate of energy loss with increasing Z. Figure 34.3 confirms this trend.',\n",
       "   'negative': 'Figure 34.14 shows that the electron critical energy for the chemical elements decreases significantly with Z.',\n",
       "   'id': 'physics_triplet_40'},\n",
       "  {'anchor': 'How is the longitudinal profile of energy deposition in an electromagnetic cascade modeled?',\n",
       "   'positive': 'The mean longitudinal profile of the energy deposition in an electromagnetic cascade is reasonably well described by a gamma distribution: {dt}=E_{0}be^{-bt}}{(a)}. }: The energy deposited per unit of scaled depth. E_0: The initial energy of the particle that started the cascade. t: The distance into the material, measured in units of radiation lengths (t = x/X_0). a, b: Fitting parameters that depend on the material and initial energy. (a): The Gamma function.',\n",
       "   'negative': 'The transverse development of electromagnetic showers in different materials scales fairly accurately with the MoliÃ¨re radius RM, given by R_{M}=X_{0}E_{s}/E_{c}.',\n",
       "   'id': 'physics_triplet_7'},\n",
       "  {'anchor': 'What is the formula for the most probable energy loss in a moderately thick detector?',\n",
       "   'positive': \"For detectors of moderate thickness, the energy loss probability distribution is described by the Landau-Vavilov distribution. The most probable energy loss is _{p}=[^{2}^{2}}{I}+{I}+j-^{2}-()], where =(K/2) Z/A z^{2}(x/^{2}) MeV. _p: The most probable energy loss in the detector. (xi): A variable that incorporates the detector's material and thickness. x: The thickness of the detector in units of mass per unit area (e.g., g cm\\\\(^{-2}\\\\)). j: A constant with a value of 0.200. m c^2: The rest mass energy of an electron (m_e c^2). Other variables (, , I, (), K, Z/A , z) have the same meanings as in the Bethe equation.\",\n",
       "   'negative': 'The mean rate of energy loss by moderately relativistic charged heavy particles is well described by the ``Bethe equation,\" -{dx}=Kz^{2}{A}{^{2}}[{2}^{2}^{2}W_{max}}{I^{2}}-^{2}-{2}].',\n",
       "   'id': 'physics_triplet_1'},\n",
       "  {'anchor': 'What does the density effect correction, (), account for?',\n",
       "   'positive': 'As the particle energy increases, its electric field flattens and extends...However, real media become polarized, limiting the field extension and effectively truncating this part of the logarithmic rise in stopping power.',\n",
       "   'negative': 'The Bloch correction z^{2}L_{2} is a low-energy correction that takes account of perturbations of the atomic wave functions.',\n",
       "   'id': 'physics_triplet_47'},\n",
       "  {'anchor': 'What is ``stopping power\"?',\n",
       "   'positive': 'The mean rate of energy loss by moderately relativistic charged heavy particles is well described by the ``Bethe equation,\" -{dx}=Kz^{2}{A}{^{2}}[{2}^{2}^{2}W_{max}}{I^{2}}-^{2}-{2}]. This is the mass stopping power. -}: The mean energy loss per unit mass area (mass stopping power). K: A constant equal to 4 N_{A}r_{e}^{2}m_{e}c^{2}. z: The charge number of the incident particle. Z: The atomic number of the absorbing material. A: The atomic mass of the absorbing material. : The particle\\'s velocity as a fraction of the speed of light (v/c). : The Lorentz factor, 1/. m_{ec^{2}}: The rest mass energy of an electron. W_{max}: The maximum possible energy transfer in a single collision. I: The mean excitation energy of the absorber. (): The density effect correction.',\n",
       "   'negative': 'Eq. (34.5) may be integrated to find the total (or partial) ``continuous slowing-down approximation\" (CSDA) range R for a particle which loses energy only through ionization and atomic excitation.',\n",
       "   'id': 'physics_triplet_17'}],\n",
       " 'total_examples': 53,\n",
       " 'results_file': 'physbert_evaluation_physbert_physics_finetuned_20250910_132516.json'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_evaluation_only()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
