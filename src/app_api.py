# app_api.py
from fastapi import FastAPI, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from typing import List, Dict, AsyncGenerator, Optional, Tuple
import time
import logging
import cProfile
import pstats
import io
import json
import asyncio
from functools import wraps

from lib.rag_service import answer_query
from lib.query_logger import get_query_logger, QueryLogEntry
from langchain.schema import Document
import lib.streamlit.session_state as ss
import lib.langchain.embeddings as langchain_embeddings
import lib.langchain.opensearch as langchain_opensearch
import lib.langchain.qdrant as langchain_qdrant
import lib.langchain.llm as langchain_llm

# Configure logging for timing
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
timing_logger = logging.getLogger("timing")


class FileUpload(BaseModel):
    name: str
    content: str


class QueryRequest(BaseModel):
    query: str
    embedding_model: str
    llm_model: str
    use_rag: bool = True
    max_documents: int = 5
    score_threshold: float = 0.0
    use_opensearch: bool = False
    use_qdrant: bool = False
    prompt: str
    files: List[FileUpload] = []
    history: List[Dict[str, str]] = []
    knowledge_base: str = "default"


class KBInspectRequest(BaseModel):
    knowledge_base: str = "default"
    embedding_model: str = "huggingface:sentence-transformers/all-mpnet-base-v2"
    use_opensearch: bool = False


app = FastAPI()

# Timing middleware to instrument request performance
@app.middleware("http")
async def timing_middleware(request: Request, call_next):
    start_time = time.time()
    timing_logger.info(f"ðŸš€ REQUEST START: {request.url.path}")
    
    response = await call_next(request)
    
    end_time = time.time()
    duration = end_time - start_time
    timing_logger.info(f"âœ… REQUEST END: {request.url.path} - Duration: {duration:.3f}s")
    
    return response

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # or specify your frontend domain like ["http://localhost:8000"]
    allow_credentials=True,
    allow_methods=["*"],  # or ["POST"]
    allow_headers=["*"],
)

# Profiling decorator
def profile_request(func):
    @wraps(func)
    def wrapper(*args, **kwargs):
        pr = cProfile.Profile()
        pr.enable()
        
        # Add detailed timing
        start_time = time.time()
        timing_logger.info(f"ðŸ“Š PROFILING START: {func.__name__}")
        
        try:
            result = func(*args, **kwargs)
            
            end_time = time.time()
            duration = end_time - start_time
            timing_logger.info(f"ðŸ“Š PROFILING END: {func.__name__} - Duration: {duration:.3f}s")
            
            pr.disable()
            
            # Generate profile stats
            s = io.StringIO()
            ps = pstats.Stats(pr, stream=s).sort_stats('cumulative')
            ps.print_stats(20)  # Top 20 functions
            timing_logger.info(f"ðŸ“ˆ PROFILE STATS:\n{s.getvalue()}")
            
            return result
            
        except Exception as e:
            pr.disable()
            timing_logger.error(f"âŒ PROFILING ERROR: {func.__name__} - {str(e)}")
            raise
            
    return wrapper

@app.post("/ask")
@profile_request
def ask(req: QueryRequest):
    user_files = [(f.name, f.content) for f in req.files]
    answer, docs = answer_query(
        query=req.query,
        embedding_model=req.embedding_model,
        llm_model=req.llm_model,
        k=req.max_documents,
        score_threshold=req.score_threshold,
        use_opensearch=req.use_opensearch,
        prompt_template=req.prompt,
        user_files=user_files,
        history=req.history,
        use_rag=req.use_rag,
        knowledge_base=req.knowledge_base,
        use_qdrant=req.use_qdrant,
    )
    # only return doc metadata, not full text
    doc_info = [
        {"id": d.id, "score": score, "snippet": d.page_content[:200]}
        for d, score in docs
    ]
    return {"answer": answer, "documents": doc_info}


@app.post("/ask/stream")
async def ask_stream(req: QueryRequest):
    """
    Streaming version of /ask endpoint using Server-Sent Events.
    Streams tokens as they are generated by the LLM for better user experience.
    Includes non-blocking logging and evaluation.
    """
    async def generate_stream() -> AsyncGenerator[str, None]:
        # Data collection for logging
        start_time = time.time()
        user_files = [(f.name, f.content) for f in req.files]
        final_answer = ""
        rag_docs = []
        retrieval_time = 0.0
        response_time = 0.0
        embedding_time = 0.0
        token_count = 0
        
        try:
            # Stream the response using Server-Sent Events format
            yield f"data: {json.dumps({'type': 'status', 'message': 'Starting document retrieval...'})}\n\n"
            
            # Import streaming function
            from lib.rag_service import answer_query_stream
            
            llm_start_time = time.time()
            
            async for chunk in answer_query_stream(
                query=req.query,
                embedding_model=req.embedding_model,
                llm_model=req.llm_model,
                k=req.max_documents,
                score_threshold=req.score_threshold,
                use_opensearch=req.use_opensearch,
                prompt_template=req.prompt,
                user_files=user_files,
                history=req.history,
                use_rag=req.use_rag,
                knowledge_base=req.knowledge_base,
                use_qdrant=req.use_qdrant,
            ):
                # Collect data from stream chunks for logging
                if chunk.get("type") == "documents":
                    retrieval_time = chunk.get("retrieval_time", 0.0)
                    # Convert document info back to the format needed for logging
                    docs_data = chunk.get("data", [])
                    for doc_info in docs_data:
                        # Create a mock Document object for logging
                        doc = Document(
                            page_content=doc_info.get("snippet", ""),
                            metadata={
                                "source": doc_info.get("source", "unknown"),
                                "page_number": doc_info.get("page", "N/A")
                            }
                        )
                        doc.id = doc_info.get("id", "unknown")
                        rag_docs.append((doc, doc_info.get("score", 0.0)))
                
                elif chunk.get("type") == "token":
                    final_answer += chunk.get("content", "")
                    token_count += 1
                    
                elif chunk.get("type") == "complete":
                    final_answer = chunk.get("final_answer", final_answer)
                    response_time = time.time() - llm_start_time
                
                # Stream the chunk to user (this is the key - user gets immediate response)
                yield f"data: {json.dumps(chunk)}\n\n"
            
            # End the stream
            yield f"data: {json.dumps({'type': 'end'})}\n\n"
            
        except Exception as e:
            # Stream the error
            error_msg = {
                "type": "error", 
                "message": str(e),
                "timestamp": time.time()
            }
            yield f"data: {json.dumps(error_msg)}\n\n"
        
        finally:
            # NON-BLOCKING LOGGING - This happens after user gets their response
            total_time = time.time() - start_time
            
            # Log ALL requests for debugging and monitoring purposes
            if True:  # Always log, regardless of success/failure/completeness
                try:
                    # Create log entry with safe defaults for missing data
                    log_entry = QueryLogEntry.create_from_api_data(
                        query=req.query or "",
                        embedding_model=req.embedding_model or "unknown",
                        llm_model=req.llm_model or "unknown", 
                        knowledge_base=req.knowledge_base or "default",
                        score_threshold=req.score_threshold or 0.0,
                        max_documents=req.max_documents or 0,
                        use_opensearch=req.use_opensearch or False,
                        use_qdrant=req.use_qdrant or False,
                        prompt_template=req.prompt or "",
                        rag_docs=rag_docs or [],
                        final_answer=final_answer or "ERROR: No response generated",
                        retrieval_time=retrieval_time or 0.0,
                        response_time=response_time or 0.0,
                        total_time=total_time,
                        embedding_time=embedding_time or 0.0,
                        token_count=token_count or 0
                    )
                    
                    # Queue for background logging (non-blocking)
                    logger_service = await get_query_logger()
                    await logger_service.log_query(log_entry)
                    
                    timing_logger.info(f"ðŸ“ QUEUED LOG: query_id={log_entry.id}, total_time={total_time:.3f}s")
                    
                except Exception as log_error:
                    # Logging failures should not affect user experience
                    timing_logger.error(f"âŒ LOGGING FAILED: {log_error}")
    
    return StreamingResponse(
        generate_stream(),
        media_type="text/plain",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",  # Disable nginx buffering
        }
    )


@app.post("/kb/count")
def count_kb_documents(req: KBInspectRequest):
    """Count documents in a knowledge base (works with both OpenSearch and in-memory)"""
    try:
        if req.use_opensearch:
            # Use OpenSearch for counting
            vs = langchain_opensearch.opensearch_doc_vector_store(
                req.embedding_model, req.knowledge_base
            )
            # Get total count by searching with match_all query
            results = vs.similarity_search("", k=1)  # Just to check if any docs exist
            # For more accurate count, we'd need to use the raw OpenSearch client
            count = len(vs.similarity_search("", k=10000))  # Rough estimate
            return {"knowledge_base": req.knowledge_base, "count": count, "storage": "opensearch"}
        else:
            # Use in-memory vector store
            vs = langchain_embeddings.get_in_memory_vector_store(
                req.embedding_model, req.knowledge_base
            )
            if vs is None:
                return {"knowledge_base": req.knowledge_base, "count": 0, "storage": "in-memory", "error": "Knowledge base not found"}
            
            # For in-memory stores, we can get documents directly
            try:
                # Try to get a large number of documents to estimate count
                docs = vs.similarity_search("", k=10000)
                count = len(docs)
            except:
                # Fallback: just indicate if KB exists
                count = "unknown"
            
            return {"knowledge_base": req.knowledge_base, "count": count, "storage": "in-memory"}
    except Exception as e:
        return {"knowledge_base": req.knowledge_base, "count": 0, "error": str(e)}


@app.post("/kb/sample")
def sample_kb_documents(req: KBInspectRequest):
    """Get sample documents from a knowledge base"""
    try:
        if req.use_opensearch:
            vs = langchain_opensearch.opensearch_doc_vector_store(
                req.embedding_model, req.knowledge_base
            )
        else:
            vs = langchain_embeddings.get_in_memory_vector_store(
                req.embedding_model, req.knowledge_base
            )
        
        if vs is None:
            return {"knowledge_base": req.knowledge_base, "documents": [], "error": "Knowledge base not found"}
        
        # Get sample documents
        docs = vs.similarity_search("", k=3)  # Get up to 3 sample docs
        
        sample_docs = []
        for doc in docs:
            sample_docs.append({
                "source": doc.metadata.get("source", "unknown"),
                "page": doc.metadata.get("page_number", "N/A"),
                "content_snippet": doc.page_content[:300] + "..." if len(doc.page_content) > 300 else doc.page_content,
                "content_length": len(doc.page_content)
            })
        
        storage_type = "opensearch" if req.use_opensearch else "in-memory"
        return {"knowledge_base": req.knowledge_base, "documents": sample_docs, "storage": storage_type}
    
    except Exception as e:
        return {"knowledge_base": req.knowledge_base, "documents": [], "error": str(e)}


@app.post("/kb/docs")
def list_kb_documents(req: KBInspectRequest):
    """List document sources in a knowledge base"""
    try:
        if req.use_opensearch:
            vs = langchain_opensearch.opensearch_doc_vector_store(
                req.embedding_model, req.knowledge_base
            )
        else:
            vs = langchain_embeddings.get_in_memory_vector_store(
                req.embedding_model, req.knowledge_base
            )
        
        if vs is None:
            return {"knowledge_base": req.knowledge_base, "documents": [], "error": "Knowledge base not found"}
        
        # Get all documents (up to a reasonable limit)
        docs = vs.similarity_search("", k=1000)
        
        # Group by source and count chunks
        sources = {}
        doc_list = []
        for doc in docs:
            source = doc.metadata.get("source", "unknown")
            sources[source] = sources.get(source, 0) + 1
            doc_list.append({
                "source": source,
                "page": doc.metadata.get("page_number", "N/A"),
                "content_length": len(doc.page_content)
            })
        
        storage_type = "opensearch" if req.use_opensearch else "in-memory"
        return {
            "knowledge_base": req.knowledge_base, 
            "documents": doc_list,
            "sources": sources,
            "total_chunks": len(docs),
            "storage": storage_type
        }
    
    except Exception as e:
        return {"knowledge_base": req.knowledge_base, "documents": [], "error": str(e)}


@app.get("/kb/list")
def list_knowledge_bases():
    """List available knowledge bases from all storage types"""
    result = {"opensearch": [], "in_memory": [], "qdrant": []}
    
    try:
        # Get OpenSearch knowledge bases
        opensearch_kbs = langchain_opensearch.get_available_knowledge_bases()
        result["opensearch"] = opensearch_kbs
    except Exception as e:
        result["opensearch_error"] = str(e)
    
    try:
        # Get in-memory knowledge bases
        in_memory_kbs = langchain_embeddings.get_in_memory_knowledge_bases()
        if not in_memory_kbs:
            in_memory_kbs = ["default"]  # Default is always available
        result["in_memory"] = in_memory_kbs
    except Exception as e:
        result["in_memory_error"] = str(e)
    
    try:
        # Get Qdrant knowledge bases
        qdrant_kbs = langchain_qdrant.get_available_knowledge_bases()
        if not qdrant_kbs:
            qdrant_kbs = ["default"]  # Default is always available
        result["qdrant"] = qdrant_kbs
    except Exception as e:
        result["qdrant_error"] = str(e)
    
    return result


# === QUERY LOGS API ENDPOINTS ===

class LogSearchRequest(BaseModel):
    query: str = "*"
    from_date: Optional[str] = None
    to_date: Optional[str] = None
    knowledge_base: Optional[str] = None
    llm_model: Optional[str] = None
    size: int = 100
    from_: int = 0


@app.post("/logs/search")
async def search_query_logs(req: LogSearchRequest):
    """Search query logs with filters"""
    try:
        logger_service = await get_query_logger()
        result = await logger_service.search_logs(
            query=req.query,
            from_date=req.from_date,
            to_date=req.to_date,
            knowledge_base=req.knowledge_base,
            llm_model=req.llm_model,
            size=req.size,
            from_=req.from_
        )
        return result
    except Exception as e:
        return {"error": str(e), "hits": [], "total": 0}


@app.get("/logs/stats")
async def get_query_log_stats():
    """Get statistics about query logs"""
    try:
        logger_service = await get_query_logger()
        
        # Get recent logs for stats
        result = await logger_service.search_logs(
            query="*",
            size=1000,
            from_=0
        )
        
        hits = result.get("hits", [])
        
        # Calculate stats
        stats = {
            "total_queries": result.get("total", 0),
            "recent_queries": len(hits),
            "status_counts": {},
            "model_counts": {},
            "knowledge_base_counts": {},
            "avg_response_time": 0.0,
            "avg_ragas_scores": {
                "faithfulness": 0.0,
                "answer_relevancy": 0.0,
                "context_precision": 0.0,
                "context_recall": 0.0
            }
        }
        
        if hits:
            response_times = []
            ragas_scores = {"faithfulness": [], "answer_relevancy": [], "context_precision": [], "context_recall": []}
            
            for hit in hits:
                source = hit["_source"]
                
                # Status counts
                status = source.get("status", "unknown")
                stats["status_counts"][status] = stats["status_counts"].get(status, 0) + 1
                
                # Model counts
                llm_model = source.get("metadata", {}).get("llm_model", "unknown")
                stats["model_counts"][llm_model] = stats["model_counts"].get(llm_model, 0) + 1
                
                # Knowledge base counts
                kb = source.get("metadata", {}).get("knowledge_base", "unknown")
                stats["knowledge_base_counts"][kb] = stats["knowledge_base_counts"].get(kb, 0) + 1
                
                # Response times
                response_time = source.get("llm_response", {}).get("response_time", 0.0)
                if response_time > 0:
                    response_times.append(response_time)
                
                # RAGAS scores
                ragas_eval = source.get("ragas_evaluation", {})
                if ragas_eval and not ragas_eval.get("error"):
                    for metric in ragas_scores.keys():
                        score = ragas_eval.get(metric)
                        if score is not None:
                            ragas_scores[metric].append(score)
            
            # Calculate averages
            if response_times:
                stats["avg_response_time"] = sum(response_times) / len(response_times)
                
            for metric, scores in ragas_scores.items():
                if scores:
                    stats["avg_ragas_scores"][metric] = sum(scores) / len(scores)
        
        return stats
        
    except Exception as e:
        return {"error": str(e)}


@app.get("/logs/export")
async def export_query_logs(
    format: str = "json",
    query: str = "*",
    from_date: Optional[str] = None,
    to_date: Optional[str] = None,
    knowledge_base: Optional[str] = None,
    llm_model: Optional[str] = None,
    size: int = 1000
):
    """Export query logs in JSON or CSV format"""
    try:
        logger_service = await get_query_logger()
        result = await logger_service.search_logs(
            query=query,
            from_date=from_date,
            to_date=to_date,
            knowledge_base=knowledge_base,
            llm_model=llm_model,
            size=size,
            from_=0
        )
        
        hits = result.get("hits", [])
        
        if format.lower() == "csv":
            import csv
            from io import StringIO
            
            # Flatten data for CSV
            csv_data = []
            for hit in hits:
                source = hit["_source"]
                row = {
                    "id": source.get("id", ""),
                    "timestamp": source.get("timestamp", ""),
                    "user_query": source.get("user_query", ""),
                    "llm_model": source.get("metadata", {}).get("llm_model", ""),
                    "embedding_model": source.get("metadata", {}).get("embedding_model", ""),
                    "knowledge_base": source.get("metadata", {}).get("knowledge_base", ""),
                    "documents_count": source.get("rag_results", {}).get("documents_count", 0),
                    "retrieval_time": source.get("rag_results", {}).get("retrieval_time", 0.0),
                    "response_time": source.get("llm_response", {}).get("response_time", 0.0),
                    "total_time": source.get("performance_metrics", {}).get("total_time", 0.0),
                    "status": source.get("status", ""),
                    "faithfulness": source.get("ragas_evaluation", {}).get("faithfulness", ""),
                    "answer_relevancy": source.get("ragas_evaluation", {}).get("answer_relevancy", ""),
                    "context_precision": source.get("ragas_evaluation", {}).get("context_precision", ""),
                    "context_recall": source.get("ragas_evaluation", {}).get("context_recall", "")
                }
                csv_data.append(row)
            
            # Generate CSV
            output = StringIO()
            if csv_data:
                writer = csv.DictWriter(output, fieldnames=csv_data[0].keys())
                writer.writeheader()
                writer.writerows(csv_data)
            
            csv_content = output.getvalue()
            
            from fastapi.responses import Response
            return Response(
                content=csv_content,
                media_type="text/csv",
                headers={"Content-Disposition": "attachment; filename=query_logs.csv"}
            )
        else:
            # Return JSON
            from fastapi.responses import Response
            return Response(
                content=json.dumps({"logs": [hit["_source"] for hit in hits], "total": result.get("total", 0)}, indent=2),
                media_type="application/json",
                headers={"Content-Disposition": "attachment; filename=query_logs.json"}
            )
            
    except Exception as e:
        return {"error": str(e)}


@app.get("/logs/health")
async def query_logs_health():
    """Health check for query logging service"""
    try:
        logger_service = await get_query_logger()
        # Try a simple search to verify connectivity
        result = await logger_service.search_logs(query="*", size=1)
        return {
            "status": "healthy",
            "opensearch_connected": True,
            "total_logs": result.get("total", 0)
        }
    except Exception as e:
        return {
            "status": "unhealthy",
            "error": str(e),
            "opensearch_connected": False
        }


@app.get("/models/embeddings")
def get_embedding_models():
    """Get list of available embedding models including fine-tuned models"""
    try:
        models = langchain_embeddings.get_available_embeddings()
        return {"models": models}
    except Exception as e:
        return {"error": str(e), "models": []}


@app.get("/models/llms")
def get_llm_models():
    """Get list of available LLM models"""
    try:
        models = langchain_llm.get_available_llms()
        return {"models": models}
    except Exception as e:
        return {"error": str(e), "models": []}


@app.get("/models/finetuned")
def get_finetuned_models():
    """Get detailed information about fine-tuned embedding models"""
    try:
        models = langchain_embeddings.get_finetuned_models()
        return {"models": models}
    except Exception as e:
        return {"error": str(e), "models": []}
